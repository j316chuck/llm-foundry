Cloning into 'llm-foundry'...
Cloned https://github.com/j316chuck/llm-foundry.git to /llm-foundry with commit abb8319
/
Processing /llm-foundry
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: mosaicml<0.29,>=0.28.0 in /usr/lib/python3/dist-packages (from mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (0.28.0)
Requirement already satisfied: mlflow<2.19,>=2.14.1 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (2.18.0)
Requirement already satisfied: accelerate<1.2,>=0.25 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (1.1.1)
Requirement already satisfied: transformers<4.47,>=4.43.2 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (4.46.3)
Requirement already satisfied: mosaicml-streaming<0.11,>=0.10.0 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (0.10.0)
Requirement already satisfied: torch<2.5.2,>=2.5.1 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (2.5.1+cu124)
Collecting datasets<2.21,>=2.20.0 (from llm-foundry==0.15.1)
  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)
Requirement already satisfied: fsspec==2023.6.0 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (2023.6.0)
Requirement already satisfied: sentencepiece==0.2.0 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (0.2.0)
Requirement already satisfied: einops==0.8.0 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (0.8.0)
Requirement already satisfied: omegaconf<3,>=2.2.3 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (2.3.0)
Requirement already satisfied: slack-sdk<4 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (3.33.5)
Requirement already satisfied: mosaicml-cli<1,>=0.6.10 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (0.6.42)
Requirement already satisfied: onnx==1.17.0 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (1.17.0)
Requirement already satisfied: onnxruntime==1.19.2 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (1.19.2)
Requirement already satisfied: boto3<2,>=1.21.45 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (1.35.77)
Requirement already satisfied: huggingface-hub<0.27,>=0.19.0 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (0.26.5)
Requirement already satisfied: beautifulsoup4<5,>=4.12.2 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (4.12.3)
Requirement already satisfied: tenacity<10,>=8.2.3 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (9.0.0)
Requirement already satisfied: catalogue<3,>=2 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (2.0.10)
Requirement already satisfied: typer<1 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (0.15.1)
Requirement already satisfied: GitPython==3.1.43 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (3.1.43)
Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/lib/python3/dist-packages (from GitPython==3.1.43->llm-foundry==0.15.1) (4.0.11)
Requirement already satisfied: numpy>=1.20 in /usr/lib/python3/dist-packages (from onnx==1.17.0->llm-foundry==0.15.1) (1.26.4)
Requirement already satisfied: protobuf>=3.20.2 in /usr/lib/python3/dist-packages (from onnx==1.17.0->llm-foundry==0.15.1) (5.29.1)
Requirement already satisfied: coloredlogs in /usr/lib/python3/dist-packages (from onnxruntime==1.19.2->llm-foundry==0.15.1) (15.0.1)
Requirement already satisfied: flatbuffers in /usr/lib/python3/dist-packages (from onnxruntime==1.19.2->llm-foundry==0.15.1) (24.3.25)
Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from onnxruntime==1.19.2->llm-foundry==0.15.1) (22.0)
Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from onnxruntime==1.19.2->llm-foundry==0.15.1) (1.13.1)
Requirement already satisfied: flash-attn==2.6.3 in /usr/lib/python3/dist-packages (from llm-foundry==0.15.1) (2.6.3)
Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate<1.2,>=0.25->llm-foundry==0.15.1) (6.1.0)
Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate<1.2,>=0.25->llm-foundry==0.15.1) (6.0.2)
Requirement already satisfied: safetensors>=0.4.3 in /usr/lib/python3/dist-packages (from accelerate<1.2,>=0.25->llm-foundry==0.15.1) (0.4.5)
Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4<5,>=4.12.2->llm-foundry==0.15.1) (2.6)
Requirement already satisfied: botocore<1.36.0,>=1.35.77 in /usr/lib/python3/dist-packages (from boto3<2,>=1.21.45->llm-foundry==0.15.1) (1.35.77)
Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/lib/python3/dist-packages (from boto3<2,>=1.21.45->llm-foundry==0.15.1) (1.0.1)
Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/lib/python3/dist-packages (from boto3<2,>=1.21.45->llm-foundry==0.15.1) (0.10.4)
Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (3.16.1)
Requirement already satisfied: pyarrow>=15.0.0 in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (16.1.0)
Collecting pyarrow-hotfix (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1)
  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (0.3.8)
Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (2.2.3)
Requirement already satisfied: requests>=2.32.2 in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (2.32.3)
Requirement already satisfied: tqdm>=4.66.3 in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (4.67.1)
Requirement already satisfied: xxhash in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (3.5.0)
Requirement already satisfied: multiprocess in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (0.70.16)
Requirement already satisfied: aiohttp in /usr/lib/python3/dist-packages (from datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (3.11.10)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<0.27,>=0.19.0->llm-foundry==0.15.1) (4.12.2)
Requirement already satisfied: mlflow-skinny==2.18.0 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (2.18.0)
Requirement already satisfied: Flask<4 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.1.0)
Requirement already satisfied: alembic!=1.10.0,<2 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.14.0)
Requirement already satisfied: docker<8,>=4.0.0 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (7.1.0)
Requirement already satisfied: graphene<4 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.4.3)
Requirement already satisfied: markdown<4,>=3.3 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.7)
Requirement already satisfied: matplotlib<4 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.9.3)
Requirement already satisfied: scikit-learn<2 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.6.0)
Requirement already satisfied: scipy<2 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.14.1)
Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (2.0.36)
Requirement already satisfied: Jinja2<4,>=2.11 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.1.4)
Requirement already satisfied: gunicorn<24 in /usr/lib/python3/dist-packages (from mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (23.0.0)
Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (5.5.0)
Requirement already satisfied: click<9,>=7.0 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (8.1.7)
Requirement already satisfied: cloudpickle<4 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.1.0)
Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (0.38.0)
Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (8.5.0)
Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.28.2)
Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.28.2)
Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/lib/python3/dist-packages (from mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (0.5.2)
Requirement already satisfied: torchmetrics<1.6.1,>=1.0 in /usr/lib/python3/dist-packages (from mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (1.6.0)
Requirement already satisfied: torch-optimizer<0.4,>=0.3.0 in /usr/lib/python3/dist-packages (from mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (0.3.0)
Requirement already satisfied: torchvision<0.20.2,>=0.18.0 in /usr/lib/python3/dist-packages (from mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (0.20.1+cu124)
Requirement already satisfied: coolname<3,>=1.1.0 in /usr/lib/python3/dist-packages (from mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (2.2.0)
Requirement already satisfied: tabulate==0.9.0 in /usr/lib/python3/dist-packages (from mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (0.9.0)
Requirement already satisfied: py-cpuinfo<10,>=8.0.0 in /usr/lib/python3/dist-packages (from mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (9.0.0)
Requirement already satisfied: pillow<12,>=10.3.0 in /usr/lib/python3/dist-packages (from mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (11.0.0)
Requirement already satisfied: argcomplete>=2.0.0 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (3.5.2)
Requirement already satisfied: arrow>=1.2.2 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (1.3.0)
Requirement already satisfied: backoff>=2.2.1 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (2.2.1)
Requirement already satisfied: gql>=3.4.0 in /usr/lib/python3/dist-packages (from gql[websockets]>=3.4.0->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (3.5.0)
Requirement already satisfied: prompt-toolkit>=3.0.29 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (3.0.36)
Requirement already satisfied: questionary>=1.10.0 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (2.0.1)
Requirement already satisfied: rich>=12.6.0 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (13.9.4)
Requirement already satisfied: ruamel.yaml>=0.17.21 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (0.18.6)
Requirement already satisfied: validators>=0.20.0 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (0.34.0)
Requirement already satisfied: urllib3>=1.23 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (2.2.3)
Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (2.5.0)
Requirement already satisfied: Brotli>=1.0.9 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.1.0)
Requirement already satisfied: google-cloud-storage<2.11.0,>=2.9.0 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.10.0)
Requirement already satisfied: paramiko<4,>=2.11.0 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (3.5.0)
Requirement already satisfied: python-snappy<1,>=0.6.1 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (0.7.3)
Requirement already satisfied: zstd<2,>=1.5.2.5 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.5.5.1)
Requirement already satisfied: oci<3,>=2.88 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.139.0)
Requirement already satisfied: azure-storage-blob<13,>=12.0.0 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (12.24.0)
Requirement already satisfied: azure-storage-file-datalake<13,>=12.11.0 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (12.18.0)
Requirement already satisfied: azure-identity>=1.13.0 in /usr/lib/python3/dist-packages (from mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.19.0)
Requirement already satisfied: apache-libcloud<4,>=3.3.1 in /usr/lib/python3/dist-packages (from mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (3.8.0)
Requirement already satisfied: pynvml<12,>=11.5.0 in /usr/lib/python3/dist-packages (from mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (11.5.3)
Requirement already satisfied: wandb<0.19,>=0.13.2 in /usr/lib/python3/dist-packages (from mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (0.18.7)
Requirement already satisfied: google-auth~=2.0 in /usr/lib/python3/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (2.36.0)
Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/lib/python3/dist-packages (from omegaconf<3,>=2.2.3->llm-foundry==0.15.1) (4.9.3)
Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (3.4.2)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (12.3.1.170)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (12.4.127)
Requirement already satisfied: triton==3.1.0 in /usr/lib/python3/dist-packages (from torch<2.5.2,>=2.5.1->llm-foundry==0.15.1) (3.1.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/lib/python3/dist-packages (from sympy->onnxruntime==1.19.2->llm-foundry==0.15.1) (1.3.0)
Requirement already satisfied: regex!=2019.12.17 in /usr/lib/python3/dist-packages (from transformers<4.47,>=4.43.2->llm-foundry==0.15.1) (2024.11.6)
Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/lib/python3/dist-packages (from transformers<4.47,>=4.43.2->llm-foundry==0.15.1) (0.20.3)
Requirement already satisfied: shellingham>=1.3.0 in /usr/lib/python3/dist-packages (from typer<1->llm-foundry==0.15.1) (1.5.4)
Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.3.8)
Requirement already satisfied: python-dateutil>=2.7.0 in /usr/lib/python3/dist-packages (from arrow>=1.2.2->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (2.9.0.post0)
Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/lib/python3/dist-packages (from arrow>=1.2.2->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (2.9.0.20241206)
Requirement already satisfied: azure-core>=1.31.0 in /usr/lib/python3/dist-packages (from azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.32.0)
Requirement already satisfied: cryptography>=2.5 in /usr/lib/python3/dist-packages (from azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (44.0.0)
Requirement already satisfied: msal>=1.30.0 in /usr/lib/python3/dist-packages (from azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.31.1)
Requirement already satisfied: msal-extensions>=1.2.0 in /usr/lib/python3/dist-packages (from azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.2.0)
Requirement already satisfied: isodate>=0.6.1 in /usr/lib/python3/dist-packages (from azure-storage-blob<13,>=12.0.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (0.7.2)
Requirement already satisfied: Werkzeug>=3.1 in /usr/lib/python3/dist-packages (from Flask<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.1.3)
Requirement already satisfied: itsdangerous>=2.2 in /usr/lib/python3/dist-packages (from Flask<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (2.2.0)
Requirement already satisfied: blinker>=1.9 in /usr/lib/python3/dist-packages (from Flask<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.9.0)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (2.4.4)
Requirement already satisfied: aiosignal>=1.1.2 in /usr/lib/python3/dist-packages (from aiohttp->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (1.3.1)
Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (24.2.0)
Requirement already satisfied: frozenlist>=1.1.1 in /usr/lib/python3/dist-packages (from aiohttp->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /usr/lib/python3/dist-packages (from aiohttp->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (6.1.0)
Requirement already satisfied: propcache>=0.2.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (0.2.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (1.18.3)
Requirement already satisfied: smmap<6,>=3.0.1 in /usr/lib/python3/dist-packages (from gitdb<5,>=4.0.1->GitPython==3.1.43->llm-foundry==0.15.1) (5.0.1)
Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/lib/python3/dist-packages (from google-cloud-storage<2.11.0,>=2.9.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.24.0)
Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/lib/python3/dist-packages (from google-cloud-storage<2.11.0,>=2.9.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.4.1)
Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/lib/python3/dist-packages (from google-cloud-storage<2.11.0,>=2.9.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.7.2)
Requirement already satisfied: graphql-core<3.3,>=3.2 in /usr/lib/python3/dist-packages (from gql>=3.4.0->gql[websockets]>=3.4.0->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (3.2.5)
Requirement already satisfied: anyio<5,>=3.0 in /usr/lib/python3/dist-packages (from gql>=3.4.0->gql[websockets]>=3.4.0->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (4.7.0)
Requirement already satisfied: websockets<12,>=10 in /usr/lib/python3/dist-packages (from gql[websockets]>=3.4.0->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (11.0.3)
Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/lib/python3/dist-packages (from graphene<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.2.0)
Requirement already satisfied: zipp>=3.20 in /usr/lib/python3/dist-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.21.0)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from Jinja2<4,>=2.11->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.0.2)
Requirement already satisfied: contourpy>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.3.1)
Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3/dist-packages (from matplotlib<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (4.55.2)
Requirement already satisfied: kiwisolver>=1.3.1 in /usr/lib/python3/dist-packages (from matplotlib<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.4.7)
Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib<4->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (2.4.7)
Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from oci<3,>=2.88->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2024.8.30)
Requirement already satisfied: pyOpenSSL<25.0.0,>=17.5.0 in /usr/lib/python3/dist-packages (from oci<3,>=2.88->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (24.3.0)
Requirement already satisfied: pytz>=2016.10 in /usr/lib/python3/dist-packages (from oci<3,>=2.88->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2024.2)
Requirement already satisfied: circuitbreaker<3.0.0,>=1.3.1 in /usr/lib/python3/dist-packages (from oci<3,>=2.88->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.0.0)
Requirement already satisfied: tzdata>=2022.7 in /usr/lib/python3/dist-packages (from pandas->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (2024.2)
Requirement already satisfied: bcrypt>=3.2 in /usr/lib/python3/dist-packages (from paramiko<4,>=2.11.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (4.2.1)
Requirement already satisfied: pynacl>=1.5 in /usr/lib/python3/dist-packages (from paramiko<4,>=2.11.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.5.0)
Requirement already satisfied: wcwidth in /usr/lib/python3/dist-packages (from prompt-toolkit>=3.0.29->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (0.2.13)
Requirement already satisfied: cramjam in /usr/lib/python3/dist-packages (from python-snappy<1,>=0.6.1->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.9.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets<2.21,>=2.20.0->llm-foundry==0.15.1) (3.10)
Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/lib/python3/dist-packages (from rich>=12.6.0->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from rich>=12.6.0->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (2.18.0)
Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/lib/python3/dist-packages (from ruamel.yaml>=0.17.21->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (0.2.12)
Requirement already satisfied: joblib>=1.2.0 in /usr/lib/python3/dist-packages (from scikit-learn<2->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/lib/python3/dist-packages (from scikit-learn<2->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.5.0)
Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (3.1.1)
Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/lib/python3/dist-packages (from torch-optimizer<0.4,>=0.3.0->mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (0.1.1)
Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/lib/python3/dist-packages (from torchmetrics<1.6.1,>=1.0->mosaicml<0.29,>=0.28.0->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (0.11.9)
Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/lib/python3/dist-packages (from wandb<0.19,>=0.13.2->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (0.4.0)
Requirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb<0.19,>=0.13.2->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (4.3.6)
Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/lib/python3/dist-packages (from wandb<0.19,>=0.13.2->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (2.19.2)
Requirement already satisfied: setproctitle in /usr/lib/python3/dist-packages (from wandb<0.19,>=0.13.2->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (1.3.4)
Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb<0.19,>=0.13.2->mosaicml[gcs,libcloud,mlflow,oci,wandb]<0.29,>=0.28.0->llm-foundry==0.15.1) (69.5.1)
Requirement already satisfied: humanfriendly>=9.1 in /usr/lib/python3/dist-packages (from coloredlogs->onnxruntime==1.19.2->llm-foundry==0.15.1) (10.0)
Requirement already satisfied: sniffio>=1.1 in /usr/lib/python3/dist-packages (from anyio<5,>=3.0->gql>=3.4.0->gql[websockets]>=3.4.0->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (1.3.1)
Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.31.0->azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.16.0)
Requirement already satisfied: cffi>=1.12 in /usr/lib/python3/dist-packages (from cryptography>=2.5->azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.17.1)
Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/lib/python3/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<2.11.0,>=2.9.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.66.0)
Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/lib/python3/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<2.11.0,>=2.9.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.25.0)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (0.4.1)
Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (4.9)
Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/lib/python3/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<2.11.0,>=2.9.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (1.6.0)
Requirement already satisfied: mdurl~=0.1 in /usr/lib/python3/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->mosaicml-cli<1,>=0.6.10->llm-foundry==0.15.1) (0.1.2)
Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/lib/python3/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.3.0)
Requirement already satisfied: portalocker<3,>=1.4 in /usr/lib/python3/dist-packages (from msal-extensions>=1.2.0->azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.10.1)
Requirement already satisfied: deprecated>=1.2.6 in /usr/lib/python3/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.2.15)
Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/lib/python3/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (0.49b2)
Requirement already satisfied: pycparser in /usr/lib/python3/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.13.0->mosaicml-streaming<0.11,>=0.10.0->llm-foundry==0.15.1) (2.22)
Requirement already satisfied: wrapt<2,>=1.10 in /usr/lib/python3/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (1.17.0)
Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/lib/python3/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.18.0->mlflow<2.19,>=2.14.1->llm-foundry==0.15.1) (0.6.1)
Downloading datasets-2.20.0-py3-none-any.whl (547 kB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/547.8 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m547.8/547.8 kB[0m [31m62.5 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)
Building wheels for collected packages: llm-foundry
  Building wheel for llm-foundry (pyproject.toml): started
  Building wheel for llm-foundry (pyproject.toml): finished with status 'done'
  Created wheel for llm-foundry: filename=llm_foundry-0.15.1-py3-none-any.whl size=324195 sha256=48d620a4365a48e78b8d6daacdfe429d35af008e68c07bc79b29967f982d5d37
  Stored in directory: /root/.cache/pip/wheels/d4/0f/6c/b8d291cded4828827a8b25c7dbd93566b736589eddcb399e31
Successfully built llm-foundry
Installing collected packages: pyarrow-hotfix, datasets, llm-foundry
  Attempting uninstall: datasets
    Found existing installation: datasets 3.1.0
    Uninstalling datasets-3.1.0:
      Successfully uninstalled datasets-3.1.0
Successfully installed datasets-2.20.0 llm-foundry-0.15.1 pyarrow-hotfix-0.6
[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.[0m[33m
[0m/
Cloning into '/workspace/bpt'...
Warning: Permanently added 'github.com' (ED25519) to the list of known hosts.
Cloned git@github.com:databricks-mosaic/bpt.git to /workspace/bpt with commit ba5e4a5
/
Cloning into '/workspace/chess_rl'...
Cloned git@github.com:j316chuck/chess_rl.git to /workspace/chess_rl with commit 678a413
/
Requirement already satisfied: absl-py in /usr/lib/python3/dist-packages (from -r requirements.txt (line 1)) (2.1.0)
Collecting apache-beam (from -r requirements.txt (line 2))
  Downloading apache_beam-2.61.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)
Collecting chess (from -r requirements.txt (line 3))
  Downloading chess-1.11.1.tar.gz (156 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting chex (from -r requirements.txt (line 4))
  Downloading chex-0.1.88-py3-none-any.whl.metadata (17 kB)
Collecting dm-haiku (from -r requirements.txt (line 5))
  Downloading dm_haiku-0.0.13-py3-none-any.whl.metadata (19 kB)
Collecting jax (from -r requirements.txt (line 6))
  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)
Collecting jaxtyping (from -r requirements.txt (line 7))
  Downloading jaxtyping-0.2.36-py3-none-any.whl.metadata (6.5 kB)
Collecting jupyter (from -r requirements.txt (line 8))
  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)
Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from -r requirements.txt (line 9)) (1.26.4)
Collecting optax (from -r requirements.txt (line 10))
  Downloading optax-0.2.4-py3-none-any.whl.metadata (8.3 kB)
Collecting orbax-checkpoint (from -r requirements.txt (line 11))
  Downloading orbax_checkpoint-0.10.3-py3-none-any.whl.metadata (1.9 kB)
Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from -r requirements.txt (line 12)) (2.2.3)
Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from -r requirements.txt (line 13)) (1.14.1)
Requirement already satisfied: typing-extensions in /usr/lib/python3/dist-packages (from -r requirements.txt (line 14)) (4.12.2)
Collecting cairosvg (from -r requirements.txt (line 15))
  Downloading CairoSVG-2.7.1-py3-none-any.whl.metadata (2.7 kB)
Collecting crcmod<2.0,>=1.7 (from apache-beam->-r requirements.txt (line 2))
  Downloading crcmod-1.7.tar.gz (89 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting orjson<4,>=3.9.7 (from apache-beam->-r requirements.txt (line 2))
  Downloading orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)
Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam->-r requirements.txt (line 2))
  Downloading dill-0.3.1.1.tar.gz (151 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting cloudpickle~=2.2.1 (from apache-beam->-r requirements.txt (line 2))
  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)
Collecting fastavro<2,>=0.23.6 (from apache-beam->-r requirements.txt (line 2))
  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)
Collecting fasteners<1.0,>=0.3 (from apache-beam->-r requirements.txt (line 2))
  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)
Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 (from apache-beam->-r requirements.txt (line 2))
  Downloading grpcio-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)
Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->-r requirements.txt (line 2))
  Downloading hdfs-2.7.3.tar.gz (43 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (0.20.2)
Collecting jsonschema<5.0.0,>=4.0.0 (from apache-beam->-r requirements.txt (line 2))
  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)
Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam->-r requirements.txt (line 2))
  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)
Collecting objsize<0.8.0,>=0.6.1 (from apache-beam->-r requirements.txt (line 2))
  Downloading objsize-0.7.0-py3-none-any.whl.metadata (12 kB)
Requirement already satisfied: packaging>=22.0 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (22.0)
Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->-r requirements.txt (line 2))
  Downloading pymongo-4.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (1.25.0)
Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (5.29.1)
Collecting pydot<2,>=1.2.0 (from apache-beam->-r requirements.txt (line 2))
  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)
Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (2.9.0.post0)
Requirement already satisfied: pytz>=2018.3 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (2024.2)
Collecting redis<6,>=5.0.0 (from apache-beam->-r requirements.txt (line 2))
  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)
Requirement already satisfied: regex>=2020.6.8 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (2024.11.6)
Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (2.32.3)
Collecting sortedcontainers>=2.4.0 (from apache-beam->-r requirements.txt (line 2))
  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)
Collecting zstandard<1,>=0.18.0 (from apache-beam->-r requirements.txt (line 2))
  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)
Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (6.0.2)
Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (16.1.0)
Requirement already satisfied: pyarrow-hotfix<1 in /usr/lib/python3/dist-packages (from apache-beam->-r requirements.txt (line 2)) (0.6)
Collecting jaxlib>=0.4.27 (from chex->-r requirements.txt (line 4))
  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)
Collecting toolz>=0.9.0 (from chex->-r requirements.txt (line 4))
  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)
Collecting jmp>=0.0.2 (from dm-haiku->-r requirements.txt (line 5))
  Downloading jmp-0.0.4-py3-none-any.whl.metadata (8.9 kB)
Requirement already satisfied: tabulate>=0.8.9 in /usr/lib/python3/dist-packages (from dm-haiku->-r requirements.txt (line 5)) (0.9.0)
Collecting ml_dtypes>=0.4.0 (from jax->-r requirements.txt (line 6))
  Downloading ml_dtypes-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)
Collecting opt_einsum (from jax->-r requirements.txt (line 6))
  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)
Collecting notebook (from jupyter->-r requirements.txt (line 8))
  Downloading notebook-7.3.2-py3-none-any.whl.metadata (10 kB)
Collecting jupyter-console (from jupyter->-r requirements.txt (line 8))
  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)
Collecting nbconvert (from jupyter->-r requirements.txt (line 8))
  Downloading nbconvert-7.16.4-py3-none-any.whl.metadata (8.5 kB)
Collecting ipykernel (from jupyter->-r requirements.txt (line 8))
  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)
Collecting ipywidgets (from jupyter->-r requirements.txt (line 8))
  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)
Collecting jupyterlab (from jupyter->-r requirements.txt (line 8))
  Downloading jupyterlab-4.3.4-py3-none-any.whl.metadata (16 kB)
Collecting etils[epy] (from optax->-r requirements.txt (line 10))
  Downloading etils-1.11.0-py3-none-any.whl.metadata (6.5 kB)
Collecting msgpack (from orbax-checkpoint->-r requirements.txt (line 11))
  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)
Collecting tensorstore>=0.1.68 (from orbax-checkpoint->-r requirements.txt (line 11))
  Downloading tensorstore-0.1.71-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)
Collecting nest_asyncio (from orbax-checkpoint->-r requirements.txt (line 11))
  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)
Collecting humanize (from orbax-checkpoint->-r requirements.txt (line 11))
  Downloading humanize-4.11.0-py3-none-any.whl.metadata (7.8 kB)
Collecting simplejson>=3.16.0 (from orbax-checkpoint->-r requirements.txt (line 11))
  Downloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)
Requirement already satisfied: tzdata>=2022.7 in /usr/lib/python3/dist-packages (from pandas->-r requirements.txt (line 12)) (2024.2)
Collecting cairocffi (from cairosvg->-r requirements.txt (line 15))
  Downloading cairocffi-1.7.1-py3-none-any.whl.metadata (3.3 kB)
Collecting cssselect2 (from cairosvg->-r requirements.txt (line 15))
  Downloading cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)
Collecting defusedxml (from cairosvg->-r requirements.txt (line 15))
  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)
Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from cairosvg->-r requirements.txt (line 15)) (11.0.0)
Collecting tinycss2 (from cairosvg->-r requirements.txt (line 15))
  Downloading tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)
Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->-r requirements.txt (line 2))
  Downloading docopt-0.6.2.tar.gz (25 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->-r requirements.txt (line 2)) (1.16.0)
Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/lib/python3/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam->-r requirements.txt (line 2)) (2.4.7)
Requirement already satisfied: attrs>=22.2.0 in /usr/lib/python3/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam->-r requirements.txt (line 2)) (24.2.0)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.0.0->apache-beam->-r requirements.txt (line 2))
  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)
Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.0.0->apache-beam->-r requirements.txt (line 2))
  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.0.0->apache-beam->-r requirements.txt (line 2))
  Downloading rpds_py-0.22.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->-r requirements.txt (line 2))
  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->-r requirements.txt (line 2)) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->-r requirements.txt (line 2)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->-r requirements.txt (line 2)) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->-r requirements.txt (line 2)) (2024.8.30)
Requirement already satisfied: cffi>=1.1.0 in /usr/lib/python3/dist-packages (from cairocffi->cairosvg->-r requirements.txt (line 15)) (1.17.1)
Collecting webencodings (from cssselect2->cairosvg->-r requirements.txt (line 15))
  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)
Requirement already satisfied: fsspec in /usr/lib/python3/dist-packages (from etils[epath,epy]->orbax-checkpoint->-r requirements.txt (line 11)) (2023.6.0)
Collecting importlib_resources (from etils[epath,epy]->orbax-checkpoint->-r requirements.txt (line 11))
  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)
Requirement already satisfied: zipp in /usr/lib/python3/dist-packages (from etils[epath,epy]->orbax-checkpoint->-r requirements.txt (line 11)) (3.21.0)
Collecting comm>=0.1.1 (from ipykernel->jupyter->-r requirements.txt (line 8))
  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)
Collecting debugpy>=1.6.5 (from ipykernel->jupyter->-r requirements.txt (line 8))
  Downloading debugpy-1.8.11-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)
Requirement already satisfied: ipython>=7.23.1 in /usr/lib/python3/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (8.30.0)
Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter->-r requirements.txt (line 8))
  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)
Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel->jupyter->-r requirements.txt (line 8))
  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)
Requirement already satisfied: matplotlib-inline>=0.1 in /usr/lib/python3/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (0.1.7)
Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (6.1.0)
Collecting pyzmq>=24 (from ipykernel->jupyter->-r requirements.txt (line 8))
  Downloading pyzmq-26.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.2 kB)
Collecting tornado>=6.1 (from ipykernel->jupyter->-r requirements.txt (line 8))
  Downloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)
Requirement already satisfied: traitlets>=5.4.0 in /usr/lib/python3/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (5.14.3)
Collecting widgetsnbextension~=4.0.12 (from ipywidgets->jupyter->-r requirements.txt (line 8))
  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)
Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets->jupyter->-r requirements.txt (line 8))
  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)
Requirement already satisfied: prompt-toolkit>=3.0.30 in /usr/lib/python3/dist-packages (from jupyter-console->jupyter->-r requirements.txt (line 8)) (3.0.36)
Requirement already satisfied: pygments in /usr/lib/python3/dist-packages (from jupyter-console->jupyter->-r requirements.txt (line 8)) (2.18.0)
Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)
Requirement already satisfied: httpx>=0.25.0 in /usr/lib/python3/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 8)) (0.28.1)
Requirement already satisfied: jinja2>=3.0.3 in /usr/lib/python3/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 8)) (3.1.4)
Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)
Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)
Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)
Collecting notebook-shim>=0.2 (from jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)
Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from jupyterlab->jupyter->-r requirements.txt (line 8)) (69.5.1)
Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (4.12.3)
Collecting bleach!=5.0.0 (from nbconvert->jupyter->-r requirements.txt (line 8))
  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)
Collecting jupyterlab-pygments (from nbconvert->jupyter->-r requirements.txt (line 8))
  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)
Requirement already satisfied: markupsafe>=2.0 in /usr/lib/python3/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (3.0.2)
Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter->-r requirements.txt (line 8))
  Downloading mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)
Collecting nbclient>=0.5.0 (from nbconvert->jupyter->-r requirements.txt (line 8))
  Downloading nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)
Collecting nbformat>=5.7 (from nbconvert->jupyter->-r requirements.txt (line 8))
  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)
Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter->-r requirements.txt (line 8))
  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)
Requirement already satisfied: pycparser in /usr/lib/python3/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg->-r requirements.txt (line 15)) (2.22)
Requirement already satisfied: anyio in /usr/lib/python3/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 8)) (4.7.0)
Requirement already satisfied: httpcore==1.* in /usr/lib/python3/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 8)) (1.0.7)
Requirement already satisfied: h11<0.15,>=0.13 in /usr/lib/python3/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 8)) (0.14.0)
Requirement already satisfied: decorator in /usr/lib/python3/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (5.1.1)
Requirement already satisfied: jedi>=0.16 in /usr/lib/python3/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (0.19.2)
Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (4.9.0)
Collecting prompt-toolkit>=3.0.30 (from jupyter-console->jupyter->-r requirements.txt (line 8))
  Downloading prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)
Requirement already satisfied: stack_data in /usr/lib/python3/dist-packages (from ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (0.6.3)
Requirement already satisfied: platformdirs>=2.5 in /usr/lib/python3/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->-r requirements.txt (line 8)) (4.3.6)
Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)
Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading jupyter_events-0.11.0-py3-none-any.whl.metadata (5.8 kB)
Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)
Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)
Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)
Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)
Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)
Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)
Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)
Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)
Collecting fastjsonschema>=2.15 (from nbformat>=5.7->nbconvert->jupyter->-r requirements.txt (line 8))
  Downloading fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)
Requirement already satisfied: wcwidth in /usr/lib/python3/dist-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->-r requirements.txt (line 8)) (0.2.13)
Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4->nbconvert->jupyter->-r requirements.txt (line 8)) (2.6)
Requirement already satisfied: sniffio>=1.1 in /usr/lib/python3/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->-r requirements.txt (line 8)) (1.3.1)
Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/lib/python3/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (0.8.4)
Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)
Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)
Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)
Requirement already satisfied: ptyprocess>=0.5 in /usr/lib/python3/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (0.7.0)
Requirement already satisfied: executing>=1.2.0 in /usr/lib/python3/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (2.1.0)
Requirement already satisfied: asttokens>=2.1.0 in /usr/lib/python3/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (2.4.1)
Requirement already satisfied: pure-eval in /usr/lib/python3/dist-packages (from stack_data->ipython>=7.23.1->ipykernel->jupyter->-r requirements.txt (line 8)) (0.2.3)
Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)
Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)
Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)
Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)
Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8))
  Downloading webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)
Requirement already satisfied: arrow>=0.15.0 in /usr/lib/python3/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8)) (1.3.0)
Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/lib/python3/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->-r requirements.txt (line 8)) (2.9.0.20241206)
Downloading apache_beam-2.61.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/16.9 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m16.9/16.9 MB[0m [31m244.0 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading chex-0.1.88-py3-none-any.whl (99 kB)
Downloading dm_haiku-0.0.13-py3-none-any.whl (373 kB)
Downloading jax-0.4.38-py3-none-any.whl (2.2 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/2.2 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.2/2.2 MB[0m [31m204.8 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading jaxtyping-0.2.36-py3-none-any.whl (55 kB)
Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)
Downloading optax-0.2.4-py3-none-any.whl (319 kB)
Downloading orbax_checkpoint-0.10.3-py3-none-any.whl (359 kB)
Downloading CairoSVG-2.7.1-py3-none-any.whl (43 kB)
Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)
Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/3.3 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.3/3.3 MB[0m [31m282.6 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)
Downloading grpcio-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/5.7 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5.7/5.7 MB[0m [31m250.6 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl (101.8 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/101.8 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m59.8/101.8 MB[0m [31m300.5 MB/s[0m eta [36m0:00:01[0m[2K   [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m101.7/101.8 MB[0m [31m299.8 MB/s[0m eta [36m0:00:01[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m101.8/101.8 MB[0m [31m224.2 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading jmp-0.0.4-py3-none-any.whl (18 kB)
Downloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)
Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)
Downloading ml_dtypes-0.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/4.5 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.5/4.5 MB[0m [31m252.0 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading objsize-0.7.0-py3-none-any.whl (11 kB)
Downloading orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)
Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)
Downloading pymongo-4.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/1.7 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/1.7 MB[0m [31m278.8 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading redis-5.2.1-py3-none-any.whl (261 kB)
Downloading simplejson-3.19.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)
Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)
Downloading tensorstore-0.1.71-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.8 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/17.8 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m17.8/17.8 MB[0m [31m259.7 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading toolz-1.0.0-py3-none-any.whl (56 kB)
Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/5.4 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5.4/5.4 MB[0m [31m333.2 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading cairocffi-1.7.1-py3-none-any.whl (75 kB)
Downloading cssselect2-0.7.0-py3-none-any.whl (15 kB)
Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)
Downloading humanize-4.11.0-py3-none-any.whl (128 kB)
Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)
Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)
Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)
Downloading jupyterlab-4.3.4-py3-none-any.whl (11.7 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/11.7 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m11.7/11.7 MB[0m [31m337.3 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)
Downloading nbconvert-7.16.4-py3-none-any.whl (257 kB)
Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)
Downloading notebook-7.3.2-py3-none-any.whl (13.2 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/13.2 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.2/13.2 MB[0m [31m316.4 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)
Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)
Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)
Downloading bleach-6.2.0-py3-none-any.whl (163 kB)
Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)
Downloading debugpy-1.8.11-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/3.1 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.1/3.1 MB[0m [31m279.2 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)
Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)
Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)
Downloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)
Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)
Downloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)
Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)
Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)
Downloading mistune-3.0.2-py3-none-any.whl (47 kB)
Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)
Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)
Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)
Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)
Downloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)
Downloading pyzmq-26.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (869 kB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/869.2 kB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m869.2/869.2 kB[0m [31m184.7 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading referencing-0.35.1-py3-none-any.whl (26 kB)
Downloading rpds_py-0.22.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)
Downloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)
Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)
Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/2.3 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.3/2.3 MB[0m [31m250.0 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading etils-1.11.0-py3-none-any.whl (165 kB)
Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)
Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)
Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)
Downloading babel-2.16.0-py3-none-any.whl (9.6 MB)
[?25l   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/9.6 MB[0m [31m?[0m eta [36m-:--:--[0m[2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m9.6/9.6 MB[0m [31m338.2 MB/s[0m eta [36m0:00:00[0m
[?25hDownloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)
Downloading json5-0.10.0-py3-none-any.whl (34 kB)
Downloading jupyter_events-0.11.0-py3-none-any.whl (19 kB)
Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)
Downloading overrides-7.7.0-py3-none-any.whl (17 kB)
Downloading prometheus_client-0.21.1-py3-none-any.whl (54 kB)
Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)
Downloading terminado-0.18.1-py3-none-any.whl (14 kB)
Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)
Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)
Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)
Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)
Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)
Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)
Downloading webcolors-24.11.1-py3-none-any.whl (14 kB)
Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)
Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)
Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)
Building wheels for collected packages: chess, crcmod, dill, hdfs, docopt
  Building wheel for chess (setup.py): started
  Building wheel for chess (setup.py): finished with status 'done'
  Created wheel for chess: filename=chess-1.11.1-py3-none-any.whl size=148497 sha256=eb698abe6fed5084efe3a94c2d80ed4b2b7a03b37dfc64ed5a32dd10cc2f50c5
  Stored in directory: /root/.cache/pip/wheels/f0/3f/76/8783033e8524d407e1bebaf72fdd3f3eba27e0c030e92bbd87
  Building wheel for crcmod (setup.py): started
  Building wheel for crcmod (setup.py): finished with status 'done'
  Created wheel for crcmod: filename=crcmod-1.7-cp311-cp311-linux_x86_64.whl size=31659 sha256=c257dca54fc69d2edb956d5ab910711c77ac5d03e66d5f8d061a5990c8407df9
  Stored in directory: /root/.cache/pip/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99
  Building wheel for dill (setup.py): started
  Building wheel for dill (setup.py): finished with status 'done'
  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78541 sha256=70189fcad39ab199cdc95678df50c0039189741ed7fbf9c465f364f96b33314f
  Stored in directory: /root/.cache/pip/wheels/01/60/80/1622338bcecce31a5664ef01c203cc5a7b09f59588d9c07376
  Building wheel for hdfs (setup.py): started
  Building wheel for hdfs (setup.py): finished with status 'done'
  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34326 sha256=9e86068a34c90f8d68dceb8b7167a5a98a8303dde3ae9b9478c070f927cd3802
  Stored in directory: /root/.cache/pip/wheels/b9/1d/dc/eb0833be25464c359903d356c4204721c6a672c26ff164cdc3
  Building wheel for docopt (setup.py): started
  Building wheel for docopt (setup.py): finished with status 'done'
  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=112d1dc634684dc6f029fbb1bbf7b4758b52e41bd39bf3a276d12ec9e5e52336
  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce
Successfully built chess crcmod dill hdfs docopt
Installing collected packages: webencodings, sortedcontainers, fastjsonschema, docopt, crcmod, zstandard, widgetsnbextension, websocket-client, webcolors, uri-template, tornado, toolz, tinycss2, simplejson, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, redis, pyzmq, python-json-logger, pydot, prompt-toolkit, prometheus-client, pandocfilters, overrides, orjson, opt_einsum, objsize, nest_asyncio, msgpack, ml_dtypes, mistune, jupyterlab-widgets, jupyterlab-pygments, jupyter-core, jsonpointer, jsonpickle, json5, jmp, jaxtyping, importlib_resources, humanize, grpcio, fqdn, fasteners, fastavro, etils, dnspython, dill, defusedxml, debugpy, comm, cloudpickle, chess, bleach, babel, async-lru, terminado, tensorstore, referencing, pymongo, jupyter-client, jaxlib, hdfs, dm-haiku, cssselect2, cairocffi, argon2-cffi-bindings, jupyter-server-terminals, jsonschema-specifications, jax, isoduration, cairosvg, argon2-cffi, orbax-checkpoint, jsonschema, ipywidgets, ipykernel, chex, optax, nbformat, jupyter-console, apache-beam, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter
  Attempting uninstall: sortedcontainers
    Found existing installation: sortedcontainers 2.1.0
    Uninstalling sortedcontainers-2.1.0:
      Successfully uninstalled sortedcontainers-2.1.0
  Attempting uninstall: prompt-toolkit
    Found existing installation: prompt-toolkit 3.0.36
    Uninstalling prompt-toolkit-3.0.36:
      Successfully uninstalled prompt-toolkit-3.0.36
  Attempting uninstall: grpcio
    Found existing installation: grpcio 1.68.1
    Uninstalling grpcio-1.68.1:
      Successfully uninstalled grpcio-1.68.1
  Attempting uninstall: dill
    Found existing installation: dill 0.3.8
    Uninstalling dill-0.3.8:
      Successfully uninstalled dill-0.3.8
  Attempting uninstall: cloudpickle
    Found existing installation: cloudpickle 3.1.0
    Uninstalling cloudpickle-3.1.0:
      Successfully uninstalled cloudpickle-3.1.0
[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.
grpcio-status 1.68.1 requires grpcio>=1.68.1, but you have grpcio 1.65.5 which is incompatible.
questionary 2.0.1 requires prompt_toolkit<=3.0.36,>=2.0, but you have prompt-toolkit 3.0.48 which is incompatible.[0m[31m
[0mSuccessfully installed apache-beam-2.61.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 async-lru-2.0.4 babel-2.16.0 bleach-6.2.0 cairocffi-1.7.1 cairosvg-2.7.1 chess-1.11.1 chex-0.1.88 cloudpickle-2.2.1 comm-0.2.2 crcmod-1.7 cssselect2-0.7.0 debugpy-1.8.11 defusedxml-0.7.1 dill-0.3.1.1 dm-haiku-0.0.13 dnspython-2.7.0 docopt-0.6.2 etils-1.11.0 fastavro-1.10.0 fasteners-0.19 fastjsonschema-2.21.1 fqdn-1.5.1 grpcio-1.65.5 hdfs-2.7.3 humanize-4.11.0 importlib_resources-6.4.5 ipykernel-6.29.5 ipywidgets-8.1.5 isoduration-20.11.0 jax-0.4.38 jaxlib-0.4.38 jaxtyping-0.2.36 jmp-0.0.4 json5-0.10.0 jsonpickle-3.4.2 jsonpointer-3.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 jupyter-1.1.1 jupyter-client-8.6.3 jupyter-console-6.6.3 jupyter-core-5.7.2 jupyter-events-0.11.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.4 jupyterlab-pygments-0.3.0 jupyterlab-server-2.27.3 jupyterlab-widgets-3.0.13 mistune-3.0.2 ml_dtypes-0.5.0 msgpack-1.1.0 nbclient-0.10.2 nbconvert-7.16.4 nbformat-5.10.4 nest_asyncio-1.6.0 notebook-7.3.2 notebook-shim-0.2.4 objsize-0.7.0 opt_einsum-3.4.0 optax-0.2.4 orbax-checkpoint-0.10.3 orjson-3.10.12 overrides-7.7.0 pandocfilters-1.5.1 prometheus-client-0.21.1 prompt-toolkit-3.0.48 pydot-1.4.2 pymongo-4.10.1 python-json-logger-3.2.1 pyzmq-26.2.0 redis-5.2.1 referencing-0.35.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.22.3 send2trash-1.8.3 simplejson-3.19.3 sortedcontainers-2.4.0 tensorstore-0.1.71 terminado-0.18.1 tinycss2-1.4.0 toolz-1.0.0 tornado-6.4.2 uri-template-1.3.0 webcolors-24.11.1 webencodings-0.5.1 websocket-client-1.8.0 widgetsnbextension-4.0.13 zstandard-0.23.0
[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.[0m[33m
[0m/
/usr/lib/python3/dist-packages/transformer_engine/pytorch/attention.py:108: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/lib/python3/dist-packages/transformer_engine/pytorch/attention.py:108: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
2024-12-28 09:25:04,645: rank0[473][MainThread]: DEBUG: llmfoundry.command_utils.train: Initializing dist with device...
2024-12-28 09:25:06,559: rank0[473][MainThread]: DEBUG: llmfoundry.command_utils.train: Testing barrier with device...
[rank0]:[W1228 09:25:06.845622010 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
NCCL version 2.21.5+cuda12.4
2024-12-28 09:25:13,019: rank0[473][MainThread]: DEBUG: llmfoundry.command_utils.train: Barrier test passed with device.
2024-12-28 09:25:13,021: rank0[473][MainThread]: INFO: llmfoundry.command_utils.train: As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
/usr/lib/python3/dist-packages/llmfoundry/utils/config_utils.py:538: UserWarning: Setting `sync_module_states = True` for FSDP. This is required when using mixed initialization.
  warnings.warn((
2024-12-28 09:25:13,021: rank0[473][MainThread]: INFO: llmfoundry.command_utils.train: Building tokenizer...
2024-12-28 09:25:15,882: rank0[473][MainThread]: INFO: llmfoundry.command_utils.train: Building train loader...
2024-12-28 09:25:15,883: rank0[473][MainThread]: DEBUG: llmfoundry.data.packing: Searching for optimal packing ratio.
2024-12-28 09:25:15,933: rank0[473][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Using packing ratio 1.0
2024-12-28 09:25:15,933: rank0[473][MainThread]: INFO: llmfoundry.data.finetuning.tasks: No preprocessor was supplied and no preprocessing function is registered for dataset name "j316chuck/chess_rl". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.
2024-12-28 09:25:15,933: rank0[473][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Dataset constructor args {'decoder_only_format': True, 'hf_kwargs': {'data_dir': 'small'}, 'max_seq_len': 8192}
Downloading readme:   0%|          | 0.00/3.35k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 3.35k/3.35k [00:00<00:00, 27.9MB/s]
Downloading data:   0%|          | 0.00/25.1M [00:00<?, ?B/s]Downloading data:  42%|████▏     | 10.5M/25.1M [00:00<00:00, 28.7MB/s]Downloading data:  83%|████████▎ | 21.0M/25.1M [00:00<00:00, 35.5MB/s]Downloading data: 100%|██████████| 25.1M/25.1M [00:00<00:00, 35.2MB/s]
Downloading data:   0%|          | 0.00/3.19M [00:00<?, ?B/s]Downloading data: 100%|██████████| 3.19M/3.19M [00:00<00:00, 26.3MB/s]Downloading data: 100%|██████████| 3.19M/3.19M [00:00<00:00, 25.8MB/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 236000 examples [00:00, 2343907.90 examples/s]Generating train split: 475000 examples [00:00, 2366847.94 examples/s]Generating train split: 500000 examples [00:00, 2360979.67 examples/s]
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 62561 examples [00:00, 2177664.42 examples/s]
Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]Process ForkPoolWorker-1:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int
Process ForkPoolWorker-2:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int
Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]
2024-12-28 09:25:22,908: rank0[473][MainThread]: DEBUG: llmfoundry.data.finetuning.tasks: Local rank 0 finished data prep
2024-12-28 09:25:34,749: rank0[473][MainThread]: ERROR: llmfoundry.data.finetuning.tasks: Error during data prep
[rank0]: ╭───────────────────── Traceback (most recent call last) ──────────────────────╮
[rank0]: │ /llm-foundry/scripts/train/train.py:9 in <module>                            │
[rank0]: │                                                                              │
[rank0]: │    6                                                                         │
[rank0]: │    7 if __name__ == '__main__':                                              │
[rank0]: │    8 │   yaml_path, args_list = sys.argv[1], sys.argv[2:]                    │
[rank0]: │ ❱  9 │   train_from_yaml(yaml_path, args_list)                               │
[rank0]: │   10                                                                         │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:662 in      │
[rank0]: │ train_from_yaml                                                              │
[rank0]: │                                                                              │
[rank0]: │   659 │   │   cli_cfg = om.from_cli(args_list)                               │
[rank0]: │   660 │   │   yaml_cfg = om.merge(yaml_cfg, cli_cfg)                         │
[rank0]: │   661 │   assert isinstance(yaml_cfg, DictConfig)                            │
[rank0]: │ ❱ 662 │   return train(yaml_cfg)                                             │
[rank0]: │   663                                                                        │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:457 in      │
[rank0]: │ train                                                                        │
[rank0]: │                                                                              │
[rank0]: │   454 │   # Dataloaders                                                      │
[rank0]: │   455 │   log.info('Building train loader...')                               │
[rank0]: │   456 │   try:                                                               │
[rank0]: │ ❱ 457 │   │   train_loader = build_dataloader(                               │
[rank0]: │   458 │   │   │   train_loader_config,                                       │
[rank0]: │   459 │   │   │   tokenizer,                                                 │
[rank0]: │   460 │   │   │   train_cfg.device_train_batch_size,                         │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank0]: │ build_dataloader                                                             │
[rank0]: │                                                                              │
[rank0]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank0]: │   37 │   }                                                                   │
[rank0]: │   38 │                                                                       │
[rank0]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank0]: │   40 │   │   name=name,                                                      │
[rank0]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank0]: │   42 │   │   partial_function=False,                                         │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank0]: │ construct_from_registry                                                      │
[rank0]: │                                                                              │
[rank0]: │   157 │   │   registered_constructor,                                        │
[rank0]: │   158 │   │   type,                                                          │
[rank0]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank0]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank0]: │   161 │   elif callable(registered_constructor):                             │
[rank0]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank0]: │   163 │   else:                                                              │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:232  │
[rank0]: │ in build_finetuning_dataloader                                               │
[rank0]: │                                                                              │
[rank0]: │   229 │   │   },                                                             │
[rank0]: │   230 │   )                                                                  │
[rank0]: │   231 │                                                                      │
[rank0]: │ ❱ 232 │   collate_fn, dataloader_batch_size = construct_from_registry(       │
[rank0]: │   233 │   │   name='finetuning_collator',                                    │
[rank0]: │   234 │   │   registry=registry.collators,                                   │
[rank0]: │   235 │   │   partial_function=False,                                        │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank0]: │ construct_from_registry                                                      │
[rank0]: │                                                                              │
[rank0]: │   157 │   │   registered_constructor,                                        │
[rank0]: │   158 │   │   type,                                                          │
[rank0]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank0]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank0]: │   161 │   elif callable(registered_constructor):                             │
[rank0]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank0]: │   163 │   else:                                                              │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/utils.py:239 in               │
[rank0]: │ get_finetuning_collator                                                      │
[rank0]: │                                                                              │
[rank0]: │   236 │   dataset_batch_size: int,                                           │
[rank0]: │   237 ) -> tuple[Union[Seq2SeqFinetuningCollator, BinPackCollator,           │
[rank0]: │   238 │   │   │   │    LossGeneratingTokensCollatorWrapper], int]:           │
[rank0]: │ ❱ 239 │   collate_fn, dataset_batch_size = build_collate_fn(                 │
[rank0]: │   240 │   │   dataloader_cfg,                                                │
[rank0]: │   241 │   │   tokenizer,                                                     │
[rank0]: │   242 │   │   dataset_batch_size,                                            │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:671  │
[rank0]: │ in build_collate_fn                                                          │
[rank0]: │                                                                              │
[rank0]: │   668 │   │   return collate_fn, device_batch_size                           │
[rank0]: │   669 │                                                                      │
[rank0]: │   670 │   if packing_ratio == 'auto':                                        │
[rank0]: │ ❱ 671 │   │   packing_ratio = auto_packing_ratio(                            │
[rank0]: │   672 │   │   │   dataloader_cfg=dataloader_cfg,                             │
[rank0]: │   673 │   │   │   tokenizer=tokenizer,                                       │
[rank0]: │   674 │   │   │   device_batch_size=device_batch_size,                       │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:411 in             │
[rank0]: │ auto_packing_ratio                                                           │
[rank0]: │                                                                              │
[rank0]: │   408 │   # Obtain the maximum packing_ratio/minimum padding that has no was │
[rank0]: │   409 │   # profiling_results are sorted from smallest to largest packing_ra │
[rank0]: │   410 │   packing_ratio = 1                                                  │
[rank0]: │ ❱ 411 │   for packing_ratio_candidate, _, waste in profiling_results:        │
[rank0]: │   412 │   │   if waste is None or waste > 0:                                 │
[rank0]: │   413 │   │   │   break                                                      │
[rank0]: │   414 │   │   packing_ratio = packing_ratio_candidate                        │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:509 in             │
[rank0]: │ profile_packing                                                              │
[rank0]: │                                                                              │
[rank0]: │   506 │                                                                      │
[rank0]: │   507 │   n_profile_examples = max(raw_batch_sizes) * 100                    │
[rank0]: │   508 │                                                                      │
[rank0]: │ ❱ 509 │   train_dataspec = build_dataloader(                                 │
[rank0]: │   510 │   │   dataloader_cfg,                                                │
[rank0]: │   511 │   │   tokenizer,                                                     │
[rank0]: │   512 │   │   n_profile_examples,                                            │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank0]: │ build_dataloader                                                             │
[rank0]: │                                                                              │
[rank0]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank0]: │   37 │   }                                                                   │
[rank0]: │   38 │                                                                       │
[rank0]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank0]: │   40 │   │   name=name,                                                      │
[rank0]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank0]: │   42 │   │   partial_function=False,                                         │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank0]: │ construct_from_registry                                                      │
[rank0]: │                                                                              │
[rank0]: │   157 │   │   registered_constructor,                                        │
[rank0]: │   158 │   │   type,                                                          │
[rank0]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank0]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank0]: │   161 │   elif callable(registered_constructor):                             │
[rank0]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank0]: │   163 │   else:                                                              │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:306  │
[rank0]: │ in build_finetuning_dataloader                                               │
[rank0]: │                                                                              │
[rank0]: │   303 │   │   │   k not in {'split', 'preprocessing_fn'}                     │
[rank0]: │   304 │   │   }                                                              │
[rank0]: │   305 │   │   log.info("Dataset constructor args %s", dataset_constructor_ar │
[rank0]: │ ❱ 306 │   │   streaming_dataset = dataset_constructor.build_from_hf(         │
[rank0]: │   307 │   │   │   dataset_name=dataset_name_or_path,                         │
[rank0]: │   308 │   │   │   split=split,                                               │
[rank0]: │   309 │   │   │   preprocessing_fn=preprocessing_fn,                         │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:1051 in   │
[rank0]: │ build_from_hf                                                                │
[rank0]: │                                                                              │
[rank0]: │   1048 │   │   │   ) from error                                              │
[rank0]: │   1049 │   │   if error is not None:                                         │
[rank0]: │   1050 │   │   │   log.error('Error during data prep')                       │
[rank0]: │ ❱ 1051 │   │   │   raise error                                               │
[rank0]: │   1052 │   │   log.debug('All ranks finished data prep')                     │
[rank0]: │   1053 │   │                                                                 │
[rank0]: │   1054 │   │   hf_tokenization_logger.removeFilter(sequence_length_warning_f │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:995 in    │
[rank0]: │ build_from_hf                                                                │
[rank0]: │                                                                              │
[rank0]: │    992 │   │   │   │   num_proc = 1                                          │
[rank0]: │    993 │   │   │                                                             │
[rank0]: │    994 │   │   │   columns_to_remove = list(dataset[0].keys())               │
[rank0]: │ ❱  995 │   │   │   tokenized_dataset = dataset.map(                          │
[rank0]: │    996 │   │   │   │   dataset_mapper,                                       │
[rank0]: │    997 │   │   │   │   batched=False,                                        │
[rank0]: │    998 │   │   │   │   remove_columns=columns_to_remove,                     │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:602 in wrapper      │
[rank0]: │                                                                              │
[rank0]: │    599 │   │   else:                                                         │
[rank0]: │    600 │   │   │   self: "Dataset" = kwargs.pop("self")                      │
[rank0]: │    601 │   │   # apply actual function                                       │
[rank0]: │ ❱  602 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank0]: │    603 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank0]: │    604 │   │   for dataset in datasets:                                      │
[rank0]: │    605 │   │   │   # Remove task templates if a column mapping of the templa │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:567 in wrapper      │
[rank0]: │                                                                              │
[rank0]: │    564 │   │   │   "output_all_columns": self._output_all_columns,           │
[rank0]: │    565 │   │   }                                                             │
[rank0]: │    566 │   │   # apply actual function                                       │
[rank0]: │ ❱  567 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank0]: │    568 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank0]: │    569 │   │   # re-apply format to the output                               │
[rank0]: │    570 │   │   for dataset in datasets:                                      │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:3253 in map         │
[rank0]: │                                                                              │
[rank0]: │   3250 │   │   │   │   │   │   total=pbar_total,                             │
[rank0]: │   3251 │   │   │   │   │   │   desc=(desc or "Map") + f" (num_proc={num_proc │
[rank0]: │   3252 │   │   │   │   │   ) as pbar:                                        │
[rank0]: │ ❱ 3253 │   │   │   │   │   │   for rank, done, content in iflatmap_unordered │
[rank0]: │   3254 │   │   │   │   │   │   │   pool, Dataset._map_single, kwargs_iterabl │
[rank0]: │   3255 │   │   │   │   │   │   ):                                            │
[rank0]: │   3256 │   │   │   │   │   │   │   if done:                                  │
[rank0]: │                                                                              │
[rank0]: │ /usr/lib/python3/dist-packages/datasets/utils/py_utils.py:711 in             │
[rank0]: │ iflatmap_unordered                                                           │
[rank0]: │                                                                              │
[rank0]: │   708 │   │   │   │   if _get_pool_pid(pool) != initial_pool_pid:            │
[rank0]: │   709 │   │   │   │   │   pool_changed = True                                │
[rank0]: │   710 │   │   │   │   │   # One of the subprocesses has died. We should not  │
[rank0]: │ ❱ 711 │   │   │   │   │   raise RuntimeError(                                │
[rank0]: │   712 │   │   │   │   │   │   "One of the subprocesses has abruptly died dur │
[rank0]: │   713 │   │   │   │   │   │   "To debug the error, disable multiprocessing." │
[rank0]: │   714 │   │   │   │   │   )                                                  │
[rank0]: ╰──────────────────────────────────────────────────────────────────────────────╯
[rank0]: RuntimeError: One of the subprocesses has abruptly died during map operation.To 
[rank0]: debug the error, disable multiprocessing.
[rank0]:[W1228 09:25:36.542033347 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
ERROR:composer.cli.launcher:Rank 6 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Global rank 0 (PID 473) exited with code 1
Global rank 1 (PID 474) exited with code 1
----------Begin global rank 1 logs----------
/usr/lib/python3/dist-packages/transformer_engine/pytorch/attention.py:108: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
2024-12-28 09:25:04,412: rank1[474][MainThread]: DEBUG: llmfoundry.command_utils.train: Initializing dist with device...
2024-12-28 09:25:05,146: rank1[474][MainThread]: DEBUG: llmfoundry.command_utils.train: Testing barrier with device...
[rank1]:[W1228 09:25:05.189808670 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
2024-12-28 09:25:13,019: rank1[474][MainThread]: DEBUG: llmfoundry.command_utils.train: Barrier test passed with device.
2024-12-28 09:25:13,020: rank1[474][MainThread]: INFO: llmfoundry.command_utils.train: As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
/usr/lib/python3/dist-packages/llmfoundry/utils/config_utils.py:538: UserWarning: Setting `sync_module_states = True` for FSDP. This is required when using mixed initialization.
  warnings.warn((
2024-12-28 09:25:13,021: rank1[474][MainThread]: INFO: llmfoundry.command_utils.train: Building tokenizer...
2024-12-28 09:25:15,892: rank1[474][MainThread]: INFO: llmfoundry.command_utils.train: Building train loader...
2024-12-28 09:25:15,893: rank1[474][MainThread]: DEBUG: llmfoundry.data.packing: Searching for optimal packing ratio.
2024-12-28 09:25:15,933: rank1[474][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Using packing ratio 1.0
2024-12-28 09:25:15,933: rank1[474][MainThread]: INFO: llmfoundry.data.finetuning.tasks: No preprocessor was supplied and no preprocessing function is registered for dataset name "j316chuck/chess_rl". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.
2024-12-28 09:25:15,933: rank1[474][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Dataset constructor args {'decoder_only_format': True, 'hf_kwargs': {'data_dir': 'small'}, 'max_seq_len': 8192}
2024-12-28 09:25:15,937: rank1[474][MainThread]: DEBUG: llmfoundry.data.finetuning.tasks: Waiting for local_rank 0 to finish data prep

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]Process ForkPoolWorker-1:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int
Process ForkPoolWorker-2:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]
2024-12-28 09:25:34,749: rank1[474][MainThread]: ERROR: llmfoundry.data.finetuning.tasks: Error during data prep
[rank1]: ╭───────────────────── Traceback (most recent call last) ──────────────────────╮
[rank1]: │ /llm-foundry/scripts/train/train.py:9 in <module>                            │
[rank1]: │                                                                              │
[rank1]: │    6                                                                         │
[rank1]: │    7 if __name__ == '__main__':                                              │
[rank1]: │    8 │   yaml_path, args_list = sys.argv[1], sys.argv[2:]                    │
[rank1]: │ ❱  9 │   train_from_yaml(yaml_path, args_list)                               │
[rank1]: │   10                                                                         │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:662 in      │
[rank1]: │ train_from_yaml                                                              │
[rank1]: │                                                                              │
[rank1]: │   659 │   │   cli_cfg = om.from_cli(args_list)                               │
[rank1]: │   660 │   │   yaml_cfg = om.merge(yaml_cfg, cli_cfg)                         │
[rank1]: │   661 │   assert isinstance(yaml_cfg, DictConfig)                            │
[rank1]: │ ❱ 662 │   return train(yaml_cfg)                                             │
[rank1]: │   663                                                                        │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:457 in      │
[rank1]: │ train                                                                        │
[rank1]: │                                                                              │
[rank1]: │   454 │   # Dataloaders                                                      │
[rank1]: │   455 │   log.info('Building train loader...')                               │
[rank1]: │   456 │   try:                                                               │
[rank1]: │ ❱ 457 │   │   train_loader = build_dataloader(                               │
[rank1]: │   458 │   │   │   train_loader_config,                                       │
[rank1]: │   459 │   │   │   tokenizer,                                                 │
[rank1]: │   460 │   │   │   train_cfg.device_train_batch_size,                         │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank1]: │ build_dataloader                                                             │
[rank1]: │                                                                              │
[rank1]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank1]: │   37 │   }                                                                   │
[rank1]: │   38 │                                                                       │
[rank1]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank1]: │   40 │   │   name=name,                                                      │
[rank1]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank1]: │   42 │   │   partial_function=False,                                         │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank1]: │ construct_from_registry                                                      │
[rank1]: │                                                                              │
[rank1]: │   157 │   │   registered_constructor,                                        │
[rank1]: │   158 │   │   type,                                                          │
[rank1]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank1]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank1]: │   161 │   elif callable(registered_constructor):                             │
[rank1]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank1]: │   163 │   else:                                                              │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:232  │
[rank1]: │ in build_finetuning_dataloader                                               │
[rank1]: │                                                                              │
[rank1]: │   229 │   │   },                                                             │
[rank1]: │   230 │   )                                                                  │
[rank1]: │   231 │                                                                      │
[rank1]: │ ❱ 232 │   collate_fn, dataloader_batch_size = construct_from_registry(       │
[rank1]: │   233 │   │   name='finetuning_collator',                                    │
[rank1]: │   234 │   │   registry=registry.collators,                                   │
[rank1]: │   235 │   │   partial_function=False,                                        │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank1]: │ construct_from_registry                                                      │
[rank1]: │                                                                              │
[rank1]: │   157 │   │   registered_constructor,                                        │
[rank1]: │   158 │   │   type,                                                          │
[rank1]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank1]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank1]: │   161 │   elif callable(registered_constructor):                             │
[rank1]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank1]: │   163 │   else:                                                              │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/utils.py:239 in               │
[rank1]: │ get_finetuning_collator                                                      │
[rank1]: │                                                                              │
[rank1]: │   236 │   dataset_batch_size: int,                                           │
[rank1]: │   237 ) -> tuple[Union[Seq2SeqFinetuningCollator, BinPackCollator,           │
[rank1]: │   238 │   │   │   │    LossGeneratingTokensCollatorWrapper], int]:           │
[rank1]: │ ❱ 239 │   collate_fn, dataset_batch_size = build_collate_fn(                 │
[rank1]: │   240 │   │   dataloader_cfg,                                                │
[rank1]: │   241 │   │   tokenizer,                                                     │
[rank1]: │   242 │   │   dataset_batch_size,                                            │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:671  │
[rank1]: │ in build_collate_fn                                                          │
[rank1]: │                                                                              │
[rank1]: │   668 │   │   return collate_fn, device_batch_size                           │
[rank1]: │   669 │                                                                      │
[rank1]: │   670 │   if packing_ratio == 'auto':                                        │
[rank1]: │ ❱ 671 │   │   packing_ratio = auto_packing_ratio(                            │
[rank1]: │   672 │   │   │   dataloader_cfg=dataloader_cfg,                             │
[rank1]: │   673 │   │   │   tokenizer=tokenizer,                                       │
[rank1]: │   674 │   │   │   device_batch_size=device_batch_size,                       │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:411 in             │
[rank1]: │ auto_packing_ratio                                                           │
[rank1]: │                                                                              │
[rank1]: │   408 │   # Obtain the maximum packing_ratio/minimum padding that has no was │
[rank1]: │   409 │   # profiling_results are sorted from smallest to largest packing_ra │
[rank1]: │   410 │   packing_ratio = 1                                                  │
[rank1]: │ ❱ 411 │   for packing_ratio_candidate, _, waste in profiling_results:        │
[rank1]: │   412 │   │   if waste is None or waste > 0:                                 │
[rank1]: │   413 │   │   │   break                                                      │
[rank1]: │   414 │   │   packing_ratio = packing_ratio_candidate                        │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:509 in             │
[rank1]: │ profile_packing                                                              │
[rank1]: │                                                                              │
[rank1]: │   506 │                                                                      │
[rank1]: │   507 │   n_profile_examples = max(raw_batch_sizes) * 100                    │
[rank1]: │   508 │                                                                      │
[rank1]: │ ❱ 509 │   train_dataspec = build_dataloader(                                 │
[rank1]: │   510 │   │   dataloader_cfg,                                                │
[rank1]: │   511 │   │   tokenizer,                                                     │
[rank1]: │   512 │   │   n_profile_examples,                                            │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank1]: │ build_dataloader                                                             │
[rank1]: │                                                                              │
[rank1]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank1]: │   37 │   }                                                                   │
[rank1]: │   38 │                                                                       │
[rank1]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank1]: │   40 │   │   name=name,                                                      │
[rank1]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank1]: │   42 │   │   partial_function=False,                                         │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank1]: │ construct_from_registry                                                      │
[rank1]: │                                                                              │
[rank1]: │   157 │   │   registered_constructor,                                        │
[rank1]: │   158 │   │   type,                                                          │
[rank1]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank1]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank1]: │   161 │   elif callable(registered_constructor):                             │
[rank1]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank1]: │   163 │   else:                                                              │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:306  │
[rank1]: │ in build_finetuning_dataloader                                               │
[rank1]: │                                                                              │
[rank1]: │   303 │   │   │   k not in {'split', 'preprocessing_fn'}                     │
[rank1]: │   304 │   │   }                                                              │
[rank1]: │   305 │   │   log.info("Dataset constructor args %s", dataset_constructor_ar │
[rank1]: │ ❱ 306 │   │   streaming_dataset = dataset_constructor.build_from_hf(         │
[rank1]: │   307 │   │   │   dataset_name=dataset_name_or_path,                         │
[rank1]: │   308 │   │   │   split=split,                                               │
[rank1]: │   309 │   │   │   preprocessing_fn=preprocessing_fn,                         │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:1051 in   │
[rank1]: │ build_from_hf                                                                │
[rank1]: │                                                                              │
[rank1]: │   1048 │   │   │   ) from error                                              │
[rank1]: │   1049 │   │   if error is not None:                                         │
[rank1]: │   1050 │   │   │   log.error('Error during data prep')                       │
[rank1]: │ ❱ 1051 │   │   │   raise error                                               │
[rank1]: │   1052 │   │   log.debug('All ranks finished data prep')                     │
[rank1]: │   1053 │   │                                                                 │
[rank1]: │   1054 │   │   hf_tokenization_logger.removeFilter(sequence_length_warning_f │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:995 in    │
[rank1]: │ build_from_hf                                                                │
[rank1]: │                                                                              │
[rank1]: │    992 │   │   │   │   num_proc = 1                                          │
[rank1]: │    993 │   │   │                                                             │
[rank1]: │    994 │   │   │   columns_to_remove = list(dataset[0].keys())               │
[rank1]: │ ❱  995 │   │   │   tokenized_dataset = dataset.map(                          │
[rank1]: │    996 │   │   │   │   dataset_mapper,                                       │
[rank1]: │    997 │   │   │   │   batched=False,                                        │
[rank1]: │    998 │   │   │   │   remove_columns=columns_to_remove,                     │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:602 in wrapper      │
[rank1]: │                                                                              │
[rank1]: │    599 │   │   else:                                                         │
[rank1]: │    600 │   │   │   self: "Dataset" = kwargs.pop("self")                      │
[rank1]: │    601 │   │   # apply actual function                                       │
[rank1]: │ ❱  602 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank1]: │    603 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank1]: │    604 │   │   for dataset in datasets:                                      │
[rank1]: │    605 │   │   │   # Remove task templates if a column mapping of the templa │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:567 in wrapper      │
[rank1]: │                                                                              │
[rank1]: │    564 │   │   │   "output_all_columns": self._output_all_columns,           │
[rank1]: │    565 │   │   }                                                             │
[rank1]: │    566 │   │   # apply actual function                                       │
[rank1]: │ ❱  567 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank1]: │    568 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank1]: │    569 │   │   # re-apply format to the output                               │
[rank1]: │    570 │   │   for dataset in datasets:                                      │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:3253 in map         │
[rank1]: │                                                                              │
[rank1]: │   3250 │   │   │   │   │   │   total=pbar_total,                             │
[rank1]: │   3251 │   │   │   │   │   │   desc=(desc or "Map") + f" (num_proc={num_proc │
[rank1]: │   3252 │   │   │   │   │   ) as pbar:                                        │
[rank1]: │ ❱ 3253 │   │   │   │   │   │   for rank, done, content in iflatmap_unordered │
[rank1]: │   3254 │   │   │   │   │   │   │   pool, Dataset._map_single, kwargs_iterabl │
[rank1]: │   3255 │   │   │   │   │   │   ):                                            │
[rank1]: │   3256 │   │   │   │   │   │   │   if done:                                  │
[rank1]: │                                                                              │
[rank1]: │ /usr/lib/python3/dist-packages/datasets/utils/py_utils.py:711 in             │
[rank1]: │ iflatmap_unordered                                                           │
[rank1]: │                                                                              │
[rank1]: │   708 │   │   │   │   if _get_pool_pid(pool) != initial_pool_pid:            │
[rank1]: │   709 │   │   │   │   │   pool_changed = True                                │
[rank1]: │   710 │   │   │   │   │   # One of the subprocesses has died. We should not  │
[rank1]: │ ❱ 711 │   │   │   │   │   raise RuntimeError(                                │
[rank1]: │   712 │   │   │   │   │   │   "One of the subprocesses has abruptly died dur │
[rank1]: │   713 │   │   │   │   │   │   "To debug the error, disable multiprocessing." │
[rank1]: │   714 │   │   │   │   │   )                                                  │
[rank1]: ╰──────────────────────────────────────────────────────────────────────────────╯
[rank1]: RuntimeError: One of the subprocesses has abruptly died during map operation.To 
[rank1]: debug the error, disable multiprocessing.

----------End global rank 1 logs----------
Global rank 2 (PID 475) exited with code 1
----------Begin global rank 2 logs----------
/usr/lib/python3/dist-packages/transformer_engine/pytorch/attention.py:108: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
2024-12-28 09:25:04,677: rank2[475][MainThread]: DEBUG: llmfoundry.command_utils.train: Initializing dist with device...
2024-12-28 09:25:05,422: rank2[475][MainThread]: DEBUG: llmfoundry.command_utils.train: Testing barrier with device...
[rank2]:[W1228 09:25:05.460205287 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
2024-12-28 09:25:13,019: rank2[475][MainThread]: DEBUG: llmfoundry.command_utils.train: Barrier test passed with device.
2024-12-28 09:25:13,020: rank2[475][MainThread]: INFO: llmfoundry.command_utils.train: As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
/usr/lib/python3/dist-packages/llmfoundry/utils/config_utils.py:538: UserWarning: Setting `sync_module_states = True` for FSDP. This is required when using mixed initialization.
  warnings.warn((
2024-12-28 09:25:13,021: rank2[475][MainThread]: INFO: llmfoundry.command_utils.train: Building tokenizer...
2024-12-28 09:25:15,859: rank2[475][MainThread]: INFO: llmfoundry.command_utils.train: Building train loader...
2024-12-28 09:25:15,860: rank2[475][MainThread]: DEBUG: llmfoundry.data.packing: Searching for optimal packing ratio.
2024-12-28 09:25:15,933: rank2[475][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Using packing ratio 1.0
2024-12-28 09:25:15,933: rank2[475][MainThread]: INFO: llmfoundry.data.finetuning.tasks: No preprocessor was supplied and no preprocessing function is registered for dataset name "j316chuck/chess_rl". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.
2024-12-28 09:25:15,933: rank2[475][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Dataset constructor args {'decoder_only_format': True, 'hf_kwargs': {'data_dir': 'small'}, 'max_seq_len': 8192}
2024-12-28 09:25:15,936: rank2[475][MainThread]: DEBUG: llmfoundry.data.finetuning.tasks: Waiting for local_rank 0 to finish data prep

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]Process ForkPoolWorker-1:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int
Process ForkPoolWorker-2:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]
2024-12-28 09:25:34,749: rank2[475][MainThread]: ERROR: llmfoundry.data.finetuning.tasks: Error during data prep
[rank2]: ╭───────────────────── Traceback (most recent call last) ──────────────────────╮
[rank2]: │ /llm-foundry/scripts/train/train.py:9 in <module>                            │
[rank2]: │                                                                              │
[rank2]: │    6                                                                         │
[rank2]: │    7 if __name__ == '__main__':                                              │
[rank2]: │    8 │   yaml_path, args_list = sys.argv[1], sys.argv[2:]                    │
[rank2]: │ ❱  9 │   train_from_yaml(yaml_path, args_list)                               │
[rank2]: │   10                                                                         │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:662 in      │
[rank2]: │ train_from_yaml                                                              │
[rank2]: │                                                                              │
[rank2]: │   659 │   │   cli_cfg = om.from_cli(args_list)                               │
[rank2]: │   660 │   │   yaml_cfg = om.merge(yaml_cfg, cli_cfg)                         │
[rank2]: │   661 │   assert isinstance(yaml_cfg, DictConfig)                            │
[rank2]: │ ❱ 662 │   return train(yaml_cfg)                                             │
[rank2]: │   663                                                                        │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:457 in      │
[rank2]: │ train                                                                        │
[rank2]: │                                                                              │
[rank2]: │   454 │   # Dataloaders                                                      │
[rank2]: │   455 │   log.info('Building train loader...')                               │
[rank2]: │   456 │   try:                                                               │
[rank2]: │ ❱ 457 │   │   train_loader = build_dataloader(                               │
[rank2]: │   458 │   │   │   train_loader_config,                                       │
[rank2]: │   459 │   │   │   tokenizer,                                                 │
[rank2]: │   460 │   │   │   train_cfg.device_train_batch_size,                         │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank2]: │ build_dataloader                                                             │
[rank2]: │                                                                              │
[rank2]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank2]: │   37 │   }                                                                   │
[rank2]: │   38 │                                                                       │
[rank2]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank2]: │   40 │   │   name=name,                                                      │
[rank2]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank2]: │   42 │   │   partial_function=False,                                         │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank2]: │ construct_from_registry                                                      │
[rank2]: │                                                                              │
[rank2]: │   157 │   │   registered_constructor,                                        │
[rank2]: │   158 │   │   type,                                                          │
[rank2]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank2]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank2]: │   161 │   elif callable(registered_constructor):                             │
[rank2]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank2]: │   163 │   else:                                                              │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:232  │
[rank2]: │ in build_finetuning_dataloader                                               │
[rank2]: │                                                                              │
[rank2]: │   229 │   │   },                                                             │
[rank2]: │   230 │   )                                                                  │
[rank2]: │   231 │                                                                      │
[rank2]: │ ❱ 232 │   collate_fn, dataloader_batch_size = construct_from_registry(       │
[rank2]: │   233 │   │   name='finetuning_collator',                                    │
[rank2]: │   234 │   │   registry=registry.collators,                                   │
[rank2]: │   235 │   │   partial_function=False,                                        │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank2]: │ construct_from_registry                                                      │
[rank2]: │                                                                              │
[rank2]: │   157 │   │   registered_constructor,                                        │
[rank2]: │   158 │   │   type,                                                          │
[rank2]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank2]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank2]: │   161 │   elif callable(registered_constructor):                             │
[rank2]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank2]: │   163 │   else:                                                              │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/utils.py:239 in               │
[rank2]: │ get_finetuning_collator                                                      │
[rank2]: │                                                                              │
[rank2]: │   236 │   dataset_batch_size: int,                                           │
[rank2]: │   237 ) -> tuple[Union[Seq2SeqFinetuningCollator, BinPackCollator,           │
[rank2]: │   238 │   │   │   │    LossGeneratingTokensCollatorWrapper], int]:           │
[rank2]: │ ❱ 239 │   collate_fn, dataset_batch_size = build_collate_fn(                 │
[rank2]: │   240 │   │   dataloader_cfg,                                                │
[rank2]: │   241 │   │   tokenizer,                                                     │
[rank2]: │   242 │   │   dataset_batch_size,                                            │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:671  │
[rank2]: │ in build_collate_fn                                                          │
[rank2]: │                                                                              │
[rank2]: │   668 │   │   return collate_fn, device_batch_size                           │
[rank2]: │   669 │                                                                      │
[rank2]: │   670 │   if packing_ratio == 'auto':                                        │
[rank2]: │ ❱ 671 │   │   packing_ratio = auto_packing_ratio(                            │
[rank2]: │   672 │   │   │   dataloader_cfg=dataloader_cfg,                             │
[rank2]: │   673 │   │   │   tokenizer=tokenizer,                                       │
[rank2]: │   674 │   │   │   device_batch_size=device_batch_size,                       │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:411 in             │
[rank2]: │ auto_packing_ratio                                                           │
[rank2]: │                                                                              │
[rank2]: │   408 │   # Obtain the maximum packing_ratio/minimum padding that has no was │
[rank2]: │   409 │   # profiling_results are sorted from smallest to largest packing_ra │
[rank2]: │   410 │   packing_ratio = 1                                                  │
[rank2]: │ ❱ 411 │   for packing_ratio_candidate, _, waste in profiling_results:        │
[rank2]: │   412 │   │   if waste is None or waste > 0:                                 │
[rank2]: │   413 │   │   │   break                                                      │
[rank2]: │   414 │   │   packing_ratio = packing_ratio_candidate                        │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:509 in             │
[rank2]: │ profile_packing                                                              │
[rank2]: │                                                                              │
[rank2]: │   506 │                                                                      │
[rank2]: │   507 │   n_profile_examples = max(raw_batch_sizes) * 100                    │
[rank2]: │   508 │                                                                      │
[rank2]: │ ❱ 509 │   train_dataspec = build_dataloader(                                 │
[rank2]: │   510 │   │   dataloader_cfg,                                                │
[rank2]: │   511 │   │   tokenizer,                                                     │
[rank2]: │   512 │   │   n_profile_examples,                                            │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank2]: │ build_dataloader                                                             │
[rank2]: │                                                                              │
[rank2]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank2]: │   37 │   }                                                                   │
[rank2]: │   38 │                                                                       │
[rank2]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank2]: │   40 │   │   name=name,                                                      │
[rank2]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank2]: │   42 │   │   partial_function=False,                                         │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank2]: │ construct_from_registry                                                      │
[rank2]: │                                                                              │
[rank2]: │   157 │   │   registered_constructor,                                        │
[rank2]: │   158 │   │   type,                                                          │
[rank2]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank2]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank2]: │   161 │   elif callable(registered_constructor):                             │
[rank2]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank2]: │   163 │   else:                                                              │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:306  │
[rank2]: │ in build_finetuning_dataloader                                               │
[rank2]: │                                                                              │
[rank2]: │   303 │   │   │   k not in {'split', 'preprocessing_fn'}                     │
[rank2]: │   304 │   │   }                                                              │
[rank2]: │   305 │   │   log.info("Dataset constructor args %s", dataset_constructor_ar │
[rank2]: │ ❱ 306 │   │   streaming_dataset = dataset_constructor.build_from_hf(         │
[rank2]: │   307 │   │   │   dataset_name=dataset_name_or_path,                         │
[rank2]: │   308 │   │   │   split=split,                                               │
[rank2]: │   309 │   │   │   preprocessing_fn=preprocessing_fn,                         │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:1051 in   │
[rank2]: │ build_from_hf                                                                │
[rank2]: │                                                                              │
[rank2]: │   1048 │   │   │   ) from error                                              │
[rank2]: │   1049 │   │   if error is not None:                                         │
[rank2]: │   1050 │   │   │   log.error('Error during data prep')                       │
[rank2]: │ ❱ 1051 │   │   │   raise error                                               │
[rank2]: │   1052 │   │   log.debug('All ranks finished data prep')                     │
[rank2]: │   1053 │   │                                                                 │
[rank2]: │   1054 │   │   hf_tokenization_logger.removeFilter(sequence_length_warning_f │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:995 in    │
[rank2]: │ build_from_hf                                                                │
[rank2]: │                                                                              │
[rank2]: │    992 │   │   │   │   num_proc = 1                                          │
[rank2]: │    993 │   │   │                                                             │
[rank2]: │    994 │   │   │   columns_to_remove = list(dataset[0].keys())               │
[rank2]: │ ❱  995 │   │   │   tokenized_dataset = dataset.map(                          │
[rank2]: │    996 │   │   │   │   dataset_mapper,                                       │
[rank2]: │    997 │   │   │   │   batched=False,                                        │
[rank2]: │    998 │   │   │   │   remove_columns=columns_to_remove,                     │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:602 in wrapper      │
[rank2]: │                                                                              │
[rank2]: │    599 │   │   else:                                                         │
[rank2]: │    600 │   │   │   self: "Dataset" = kwargs.pop("self")                      │
[rank2]: │    601 │   │   # apply actual function                                       │
[rank2]: │ ❱  602 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank2]: │    603 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank2]: │    604 │   │   for dataset in datasets:                                      │
[rank2]: │    605 │   │   │   # Remove task templates if a column mapping of the templa │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:567 in wrapper      │
[rank2]: │                                                                              │
[rank2]: │    564 │   │   │   "output_all_columns": self._output_all_columns,           │
[rank2]: │    565 │   │   }                                                             │
[rank2]: │    566 │   │   # apply actual function                                       │
[rank2]: │ ❱  567 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank2]: │    568 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank2]: │    569 │   │   # re-apply format to the output                               │
[rank2]: │    570 │   │   for dataset in datasets:                                      │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:3253 in map         │
[rank2]: │                                                                              │
[rank2]: │   3250 │   │   │   │   │   │   total=pbar_total,                             │
[rank2]: │   3251 │   │   │   │   │   │   desc=(desc or "Map") + f" (num_proc={num_proc │
[rank2]: │   3252 │   │   │   │   │   ) as pbar:                                        │
[rank2]: │ ❱ 3253 │   │   │   │   │   │   for rank, done, content in iflatmap_unordered │
[rank2]: │   3254 │   │   │   │   │   │   │   pool, Dataset._map_single, kwargs_iterabl │
[rank2]: │   3255 │   │   │   │   │   │   ):                                            │
[rank2]: │   3256 │   │   │   │   │   │   │   if done:                                  │
[rank2]: │                                                                              │
[rank2]: │ /usr/lib/python3/dist-packages/datasets/utils/py_utils.py:711 in             │
[rank2]: │ iflatmap_unordered                                                           │
[rank2]: │                                                                              │
[rank2]: │   708 │   │   │   │   if _get_pool_pid(pool) != initial_pool_pid:            │
[rank2]: │   709 │   │   │   │   │   pool_changed = True                                │
[rank2]: │   710 │   │   │   │   │   # One of the subprocesses has died. We should not  │
[rank2]: │ ❱ 711 │   │   │   │   │   raise RuntimeError(                                │
[rank2]: │   712 │   │   │   │   │   │   "One of the subprocesses has abruptly died dur │
[rank2]: │   713 │   │   │   │   │   │   "To debug the error, disable multiprocessing." │
[rank2]: │   714 │   │   │   │   │   )                                                  │
[rank2]: ╰──────────────────────────────────────────────────────────────────────────────╯
[rank2]: RuntimeError: One of the subprocesses has abruptly died during map operation.To 
[rank2]: debug the error, disable multiprocessing.

----------End global rank 2 logs----------
Global rank 3 (PID 476) exited with code 1
----------Begin global rank 3 logs----------
/usr/lib/python3/dist-packages/transformer_engine/pytorch/attention.py:108: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
2024-12-28 09:25:00,244: rank3[476][MainThread]: DEBUG: llmfoundry.command_utils.train: Initializing dist with device...
2024-12-28 09:25:05,808: rank3[476][MainThread]: DEBUG: llmfoundry.command_utils.train: Testing barrier with device...
[rank3]:[W1228 09:25:05.846431730 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
2024-12-28 09:25:13,019: rank3[476][MainThread]: DEBUG: llmfoundry.command_utils.train: Barrier test passed with device.
2024-12-28 09:25:13,020: rank3[476][MainThread]: INFO: llmfoundry.command_utils.train: As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
/usr/lib/python3/dist-packages/llmfoundry/utils/config_utils.py:538: UserWarning: Setting `sync_module_states = True` for FSDP. This is required when using mixed initialization.
  warnings.warn((
2024-12-28 09:25:13,021: rank3[476][MainThread]: INFO: llmfoundry.command_utils.train: Building tokenizer...
2024-12-28 09:25:15,865: rank3[476][MainThread]: INFO: llmfoundry.command_utils.train: Building train loader...
2024-12-28 09:25:15,866: rank3[476][MainThread]: DEBUG: llmfoundry.data.packing: Searching for optimal packing ratio.
2024-12-28 09:25:15,933: rank3[476][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Using packing ratio 1.0
2024-12-28 09:25:15,933: rank3[476][MainThread]: INFO: llmfoundry.data.finetuning.tasks: No preprocessor was supplied and no preprocessing function is registered for dataset name "j316chuck/chess_rl". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.
2024-12-28 09:25:15,933: rank3[476][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Dataset constructor args {'decoder_only_format': True, 'hf_kwargs': {'data_dir': 'small'}, 'max_seq_len': 8192}
2024-12-28 09:25:15,936: rank3[476][MainThread]: DEBUG: llmfoundry.data.finetuning.tasks: Waiting for local_rank 0 to finish data prep

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]Process ForkPoolWorker-1:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int
Process ForkPoolWorker-2:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]
2024-12-28 09:25:34,749: rank3[476][MainThread]: ERROR: llmfoundry.data.finetuning.tasks: Error during data prep
[rank3]: ╭───────────────────── Traceback (most recent call last) ──────────────────────╮
[rank3]: │ /llm-foundry/scripts/train/train.py:9 in <module>                            │
[rank3]: │                                                                              │
[rank3]: │    6                                                                         │
[rank3]: │    7 if __name__ == '__main__':                                              │
[rank3]: │    8 │   yaml_path, args_list = sys.argv[1], sys.argv[2:]                    │
[rank3]: │ ❱  9 │   train_from_yaml(yaml_path, args_list)                               │
[rank3]: │   10                                                                         │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:662 in      │
[rank3]: │ train_from_yaml                                                              │
[rank3]: │                                                                              │
[rank3]: │   659 │   │   cli_cfg = om.from_cli(args_list)                               │
[rank3]: │   660 │   │   yaml_cfg = om.merge(yaml_cfg, cli_cfg)                         │
[rank3]: │   661 │   assert isinstance(yaml_cfg, DictConfig)                            │
[rank3]: │ ❱ 662 │   return train(yaml_cfg)                                             │
[rank3]: │   663                                                                        │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:457 in      │
[rank3]: │ train                                                                        │
[rank3]: │                                                                              │
[rank3]: │   454 │   # Dataloaders                                                      │
[rank3]: │   455 │   log.info('Building train loader...')                               │
[rank3]: │   456 │   try:                                                               │
[rank3]: │ ❱ 457 │   │   train_loader = build_dataloader(                               │
[rank3]: │   458 │   │   │   train_loader_config,                                       │
[rank3]: │   459 │   │   │   tokenizer,                                                 │
[rank3]: │   460 │   │   │   train_cfg.device_train_batch_size,                         │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank3]: │ build_dataloader                                                             │
[rank3]: │                                                                              │
[rank3]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank3]: │   37 │   }                                                                   │
[rank3]: │   38 │                                                                       │
[rank3]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank3]: │   40 │   │   name=name,                                                      │
[rank3]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank3]: │   42 │   │   partial_function=False,                                         │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank3]: │ construct_from_registry                                                      │
[rank3]: │                                                                              │
[rank3]: │   157 │   │   registered_constructor,                                        │
[rank3]: │   158 │   │   type,                                                          │
[rank3]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank3]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank3]: │   161 │   elif callable(registered_constructor):                             │
[rank3]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank3]: │   163 │   else:                                                              │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:232  │
[rank3]: │ in build_finetuning_dataloader                                               │
[rank3]: │                                                                              │
[rank3]: │   229 │   │   },                                                             │
[rank3]: │   230 │   )                                                                  │
[rank3]: │   231 │                                                                      │
[rank3]: │ ❱ 232 │   collate_fn, dataloader_batch_size = construct_from_registry(       │
[rank3]: │   233 │   │   name='finetuning_collator',                                    │
[rank3]: │   234 │   │   registry=registry.collators,                                   │
[rank3]: │   235 │   │   partial_function=False,                                        │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank3]: │ construct_from_registry                                                      │
[rank3]: │                                                                              │
[rank3]: │   157 │   │   registered_constructor,                                        │
[rank3]: │   158 │   │   type,                                                          │
[rank3]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank3]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank3]: │   161 │   elif callable(registered_constructor):                             │
[rank3]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank3]: │   163 │   else:                                                              │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/utils.py:239 in               │
[rank3]: │ get_finetuning_collator                                                      │
[rank3]: │                                                                              │
[rank3]: │   236 │   dataset_batch_size: int,                                           │
[rank3]: │   237 ) -> tuple[Union[Seq2SeqFinetuningCollator, BinPackCollator,           │
[rank3]: │   238 │   │   │   │    LossGeneratingTokensCollatorWrapper], int]:           │
[rank3]: │ ❱ 239 │   collate_fn, dataset_batch_size = build_collate_fn(                 │
[rank3]: │   240 │   │   dataloader_cfg,                                                │
[rank3]: │   241 │   │   tokenizer,                                                     │
[rank3]: │   242 │   │   dataset_batch_size,                                            │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:671  │
[rank3]: │ in build_collate_fn                                                          │
[rank3]: │                                                                              │
[rank3]: │   668 │   │   return collate_fn, device_batch_size                           │
[rank3]: │   669 │                                                                      │
[rank3]: │   670 │   if packing_ratio == 'auto':                                        │
[rank3]: │ ❱ 671 │   │   packing_ratio = auto_packing_ratio(                            │
[rank3]: │   672 │   │   │   dataloader_cfg=dataloader_cfg,                             │
[rank3]: │   673 │   │   │   tokenizer=tokenizer,                                       │
[rank3]: │   674 │   │   │   device_batch_size=device_batch_size,                       │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:411 in             │
[rank3]: │ auto_packing_ratio                                                           │
[rank3]: │                                                                              │
[rank3]: │   408 │   # Obtain the maximum packing_ratio/minimum padding that has no was │
[rank3]: │   409 │   # profiling_results are sorted from smallest to largest packing_ra │
[rank3]: │   410 │   packing_ratio = 1                                                  │
[rank3]: │ ❱ 411 │   for packing_ratio_candidate, _, waste in profiling_results:        │
[rank3]: │   412 │   │   if waste is None or waste > 0:                                 │
[rank3]: │   413 │   │   │   break                                                      │
[rank3]: │   414 │   │   packing_ratio = packing_ratio_candidate                        │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:509 in             │
[rank3]: │ profile_packing                                                              │
[rank3]: │                                                                              │
[rank3]: │   506 │                                                                      │
[rank3]: │   507 │   n_profile_examples = max(raw_batch_sizes) * 100                    │
[rank3]: │   508 │                                                                      │
[rank3]: │ ❱ 509 │   train_dataspec = build_dataloader(                                 │
[rank3]: │   510 │   │   dataloader_cfg,                                                │
[rank3]: │   511 │   │   tokenizer,                                                     │
[rank3]: │   512 │   │   n_profile_examples,                                            │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank3]: │ build_dataloader                                                             │
[rank3]: │                                                                              │
[rank3]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank3]: │   37 │   }                                                                   │
[rank3]: │   38 │                                                                       │
[rank3]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank3]: │   40 │   │   name=name,                                                      │
[rank3]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank3]: │   42 │   │   partial_function=False,                                         │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank3]: │ construct_from_registry                                                      │
[rank3]: │                                                                              │
[rank3]: │   157 │   │   registered_constructor,                                        │
[rank3]: │   158 │   │   type,                                                          │
[rank3]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank3]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank3]: │   161 │   elif callable(registered_constructor):                             │
[rank3]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank3]: │   163 │   else:                                                              │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:306  │
[rank3]: │ in build_finetuning_dataloader                                               │
[rank3]: │                                                                              │
[rank3]: │   303 │   │   │   k not in {'split', 'preprocessing_fn'}                     │
[rank3]: │   304 │   │   }                                                              │
[rank3]: │   305 │   │   log.info("Dataset constructor args %s", dataset_constructor_ar │
[rank3]: │ ❱ 306 │   │   streaming_dataset = dataset_constructor.build_from_hf(         │
[rank3]: │   307 │   │   │   dataset_name=dataset_name_or_path,                         │
[rank3]: │   308 │   │   │   split=split,                                               │
[rank3]: │   309 │   │   │   preprocessing_fn=preprocessing_fn,                         │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:1051 in   │
[rank3]: │ build_from_hf                                                                │
[rank3]: │                                                                              │
[rank3]: │   1048 │   │   │   ) from error                                              │
[rank3]: │   1049 │   │   if error is not None:                                         │
[rank3]: │   1050 │   │   │   log.error('Error during data prep')                       │
[rank3]: │ ❱ 1051 │   │   │   raise error                                               │
[rank3]: │   1052 │   │   log.debug('All ranks finished data prep')                     │
[rank3]: │   1053 │   │                                                                 │
[rank3]: │   1054 │   │   hf_tokenization_logger.removeFilter(sequence_length_warning_f │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:995 in    │
[rank3]: │ build_from_hf                                                                │
[rank3]: │                                                                              │
[rank3]: │    992 │   │   │   │   num_proc = 1                                          │
[rank3]: │    993 │   │   │                                                             │
[rank3]: │    994 │   │   │   columns_to_remove = list(dataset[0].keys())               │
[rank3]: │ ❱  995 │   │   │   tokenized_dataset = dataset.map(                          │
[rank3]: │    996 │   │   │   │   dataset_mapper,                                       │
[rank3]: │    997 │   │   │   │   batched=False,                                        │
[rank3]: │    998 │   │   │   │   remove_columns=columns_to_remove,                     │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:602 in wrapper      │
[rank3]: │                                                                              │
[rank3]: │    599 │   │   else:                                                         │
[rank3]: │    600 │   │   │   self: "Dataset" = kwargs.pop("self")                      │
[rank3]: │    601 │   │   # apply actual function                                       │
[rank3]: │ ❱  602 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank3]: │    603 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank3]: │    604 │   │   for dataset in datasets:                                      │
[rank3]: │    605 │   │   │   # Remove task templates if a column mapping of the templa │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:567 in wrapper      │
[rank3]: │                                                                              │
[rank3]: │    564 │   │   │   "output_all_columns": self._output_all_columns,           │
[rank3]: │    565 │   │   }                                                             │
[rank3]: │    566 │   │   # apply actual function                                       │
[rank3]: │ ❱  567 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank3]: │    568 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank3]: │    569 │   │   # re-apply format to the output                               │
[rank3]: │    570 │   │   for dataset in datasets:                                      │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:3253 in map         │
[rank3]: │                                                                              │
[rank3]: │   3250 │   │   │   │   │   │   total=pbar_total,                             │
[rank3]: │   3251 │   │   │   │   │   │   desc=(desc or "Map") + f" (num_proc={num_proc │
[rank3]: │   3252 │   │   │   │   │   ) as pbar:                                        │
[rank3]: │ ❱ 3253 │   │   │   │   │   │   for rank, done, content in iflatmap_unordered │
[rank3]: │   3254 │   │   │   │   │   │   │   pool, Dataset._map_single, kwargs_iterabl │
[rank3]: │   3255 │   │   │   │   │   │   ):                                            │
[rank3]: │   3256 │   │   │   │   │   │   │   if done:                                  │
[rank3]: │                                                                              │
[rank3]: │ /usr/lib/python3/dist-packages/datasets/utils/py_utils.py:711 in             │
[rank3]: │ iflatmap_unordered                                                           │
[rank3]: │                                                                              │
[rank3]: │   708 │   │   │   │   if _get_pool_pid(pool) != initial_pool_pid:            │
[rank3]: │   709 │   │   │   │   │   pool_changed = True                                │
[rank3]: │   710 │   │   │   │   │   # One of the subprocesses has died. We should not  │
[rank3]: │ ❱ 711 │   │   │   │   │   raise RuntimeError(                                │
[rank3]: │   712 │   │   │   │   │   │   "One of the subprocesses has abruptly died dur │
[rank3]: │   713 │   │   │   │   │   │   "To debug the error, disable multiprocessing." │
[rank3]: │   714 │   │   │   │   │   )                                                  │
[rank3]: ╰──────────────────────────────────────────────────────────────────────────────╯
[rank3]: RuntimeError: One of the subprocesses has abruptly died during map operation.To 
[rank3]: debug the error, disable multiprocessing.

----------End global rank 3 logs----------
Global rank 4 (PID 477) exited with code 1
----------Begin global rank 4 logs----------
/usr/lib/python3/dist-packages/transformer_engine/pytorch/attention.py:108: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
2024-12-28 09:25:03,483: rank4[477][MainThread]: DEBUG: llmfoundry.command_utils.train: Initializing dist with device...
2024-12-28 09:25:04,730: rank4[477][MainThread]: DEBUG: llmfoundry.command_utils.train: Testing barrier with device...
[rank4]:[W1228 09:25:04.789237682 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
2024-12-28 09:25:13,019: rank4[477][MainThread]: DEBUG: llmfoundry.command_utils.train: Barrier test passed with device.
2024-12-28 09:25:13,021: rank4[477][MainThread]: INFO: llmfoundry.command_utils.train: As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
/usr/lib/python3/dist-packages/llmfoundry/utils/config_utils.py:538: UserWarning: Setting `sync_module_states = True` for FSDP. This is required when using mixed initialization.
  warnings.warn((
2024-12-28 09:25:13,021: rank4[477][MainThread]: INFO: llmfoundry.command_utils.train: Building tokenizer...
2024-12-28 09:25:15,827: rank4[477][MainThread]: INFO: llmfoundry.command_utils.train: Building train loader...
2024-12-28 09:25:15,828: rank4[477][MainThread]: DEBUG: llmfoundry.data.packing: Searching for optimal packing ratio.
2024-12-28 09:25:15,933: rank4[477][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Using packing ratio 1.0
2024-12-28 09:25:15,933: rank4[477][MainThread]: INFO: llmfoundry.data.finetuning.tasks: No preprocessor was supplied and no preprocessing function is registered for dataset name "j316chuck/chess_rl". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.
2024-12-28 09:25:15,933: rank4[477][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Dataset constructor args {'decoder_only_format': True, 'hf_kwargs': {'data_dir': 'small'}, 'max_seq_len': 8192}
2024-12-28 09:25:15,937: rank4[477][MainThread]: DEBUG: llmfoundry.data.finetuning.tasks: Waiting for local_rank 0 to finish data prep

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]Process ForkPoolWorker-1:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int
Process ForkPoolWorker-2:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]
2024-12-28 09:25:34,749: rank4[477][MainThread]: ERROR: llmfoundry.data.finetuning.tasks: Error during data prep
[rank4]: ╭───────────────────── Traceback (most recent call last) ──────────────────────╮
[rank4]: │ /llm-foundry/scripts/train/train.py:9 in <module>                            │
[rank4]: │                                                                              │
[rank4]: │    6                                                                         │
[rank4]: │    7 if __name__ == '__main__':                                              │
[rank4]: │    8 │   yaml_path, args_list = sys.argv[1], sys.argv[2:]                    │
[rank4]: │ ❱  9 │   train_from_yaml(yaml_path, args_list)                               │
[rank4]: │   10                                                                         │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:662 in      │
[rank4]: │ train_from_yaml                                                              │
[rank4]: │                                                                              │
[rank4]: │   659 │   │   cli_cfg = om.from_cli(args_list)                               │
[rank4]: │   660 │   │   yaml_cfg = om.merge(yaml_cfg, cli_cfg)                         │
[rank4]: │   661 │   assert isinstance(yaml_cfg, DictConfig)                            │
[rank4]: │ ❱ 662 │   return train(yaml_cfg)                                             │
[rank4]: │   663                                                                        │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:457 in      │
[rank4]: │ train                                                                        │
[rank4]: │                                                                              │
[rank4]: │   454 │   # Dataloaders                                                      │
[rank4]: │   455 │   log.info('Building train loader...')                               │
[rank4]: │   456 │   try:                                                               │
[rank4]: │ ❱ 457 │   │   train_loader = build_dataloader(                               │
[rank4]: │   458 │   │   │   train_loader_config,                                       │
[rank4]: │   459 │   │   │   tokenizer,                                                 │
[rank4]: │   460 │   │   │   train_cfg.device_train_batch_size,                         │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank4]: │ build_dataloader                                                             │
[rank4]: │                                                                              │
[rank4]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank4]: │   37 │   }                                                                   │
[rank4]: │   38 │                                                                       │
[rank4]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank4]: │   40 │   │   name=name,                                                      │
[rank4]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank4]: │   42 │   │   partial_function=False,                                         │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank4]: │ construct_from_registry                                                      │
[rank4]: │                                                                              │
[rank4]: │   157 │   │   registered_constructor,                                        │
[rank4]: │   158 │   │   type,                                                          │
[rank4]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank4]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank4]: │   161 │   elif callable(registered_constructor):                             │
[rank4]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank4]: │   163 │   else:                                                              │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:232  │
[rank4]: │ in build_finetuning_dataloader                                               │
[rank4]: │                                                                              │
[rank4]: │   229 │   │   },                                                             │
[rank4]: │   230 │   )                                                                  │
[rank4]: │   231 │                                                                      │
[rank4]: │ ❱ 232 │   collate_fn, dataloader_batch_size = construct_from_registry(       │
[rank4]: │   233 │   │   name='finetuning_collator',                                    │
[rank4]: │   234 │   │   registry=registry.collators,                                   │
[rank4]: │   235 │   │   partial_function=False,                                        │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank4]: │ construct_from_registry                                                      │
[rank4]: │                                                                              │
[rank4]: │   157 │   │   registered_constructor,                                        │
[rank4]: │   158 │   │   type,                                                          │
[rank4]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank4]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank4]: │   161 │   elif callable(registered_constructor):                             │
[rank4]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank4]: │   163 │   else:                                                              │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/utils.py:239 in               │
[rank4]: │ get_finetuning_collator                                                      │
[rank4]: │                                                                              │
[rank4]: │   236 │   dataset_batch_size: int,                                           │
[rank4]: │   237 ) -> tuple[Union[Seq2SeqFinetuningCollator, BinPackCollator,           │
[rank4]: │   238 │   │   │   │    LossGeneratingTokensCollatorWrapper], int]:           │
[rank4]: │ ❱ 239 │   collate_fn, dataset_batch_size = build_collate_fn(                 │
[rank4]: │   240 │   │   dataloader_cfg,                                                │
[rank4]: │   241 │   │   tokenizer,                                                     │
[rank4]: │   242 │   │   dataset_batch_size,                                            │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:671  │
[rank4]: │ in build_collate_fn                                                          │
[rank4]: │                                                                              │
[rank4]: │   668 │   │   return collate_fn, device_batch_size                           │
[rank4]: │   669 │                                                                      │
[rank4]: │   670 │   if packing_ratio == 'auto':                                        │
[rank4]: │ ❱ 671 │   │   packing_ratio = auto_packing_ratio(                            │
[rank4]: │   672 │   │   │   dataloader_cfg=dataloader_cfg,                             │
[rank4]: │   673 │   │   │   tokenizer=tokenizer,                                       │
[rank4]: │   674 │   │   │   device_batch_size=device_batch_size,                       │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:411 in             │
[rank4]: │ auto_packing_ratio                                                           │
[rank4]: │                                                                              │
[rank4]: │   408 │   # Obtain the maximum packing_ratio/minimum padding that has no was │
[rank4]: │   409 │   # profiling_results are sorted from smallest to largest packing_ra │
[rank4]: │   410 │   packing_ratio = 1                                                  │
[rank4]: │ ❱ 411 │   for packing_ratio_candidate, _, waste in profiling_results:        │
[rank4]: │   412 │   │   if waste is None or waste > 0:                                 │
[rank4]: │   413 │   │   │   break                                                      │
[rank4]: │   414 │   │   packing_ratio = packing_ratio_candidate                        │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:509 in             │
[rank4]: │ profile_packing                                                              │
[rank4]: │                                                                              │
[rank4]: │   506 │                                                                      │
[rank4]: │   507 │   n_profile_examples = max(raw_batch_sizes) * 100                    │
[rank4]: │   508 │                                                                      │
[rank4]: │ ❱ 509 │   train_dataspec = build_dataloader(                                 │
[rank4]: │   510 │   │   dataloader_cfg,                                                │
[rank4]: │   511 │   │   tokenizer,                                                     │
[rank4]: │   512 │   │   n_profile_examples,                                            │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank4]: │ build_dataloader                                                             │
[rank4]: │                                                                              │
[rank4]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank4]: │   37 │   }                                                                   │
[rank4]: │   38 │                                                                       │
[rank4]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank4]: │   40 │   │   name=name,                                                      │
[rank4]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank4]: │   42 │   │   partial_function=False,                                         │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank4]: │ construct_from_registry                                                      │
[rank4]: │                                                                              │
[rank4]: │   157 │   │   registered_constructor,                                        │
[rank4]: │   158 │   │   type,                                                          │
[rank4]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank4]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank4]: │   161 │   elif callable(registered_constructor):                             │
[rank4]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank4]: │   163 │   else:                                                              │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:306  │
[rank4]: │ in build_finetuning_dataloader                                               │
[rank4]: │                                                                              │
[rank4]: │   303 │   │   │   k not in {'split', 'preprocessing_fn'}                     │
[rank4]: │   304 │   │   }                                                              │
[rank4]: │   305 │   │   log.info("Dataset constructor args %s", dataset_constructor_ar │
[rank4]: │ ❱ 306 │   │   streaming_dataset = dataset_constructor.build_from_hf(         │
[rank4]: │   307 │   │   │   dataset_name=dataset_name_or_path,                         │
[rank4]: │   308 │   │   │   split=split,                                               │
[rank4]: │   309 │   │   │   preprocessing_fn=preprocessing_fn,                         │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:1051 in   │
[rank4]: │ build_from_hf                                                                │
[rank4]: │                                                                              │
[rank4]: │   1048 │   │   │   ) from error                                              │
[rank4]: │   1049 │   │   if error is not None:                                         │
[rank4]: │   1050 │   │   │   log.error('Error during data prep')                       │
[rank4]: │ ❱ 1051 │   │   │   raise error                                               │
[rank4]: │   1052 │   │   log.debug('All ranks finished data prep')                     │
[rank4]: │   1053 │   │                                                                 │
[rank4]: │   1054 │   │   hf_tokenization_logger.removeFilter(sequence_length_warning_f │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:995 in    │
[rank4]: │ build_from_hf                                                                │
[rank4]: │                                                                              │
[rank4]: │    992 │   │   │   │   num_proc = 1                                          │
[rank4]: │    993 │   │   │                                                             │
[rank4]: │    994 │   │   │   columns_to_remove = list(dataset[0].keys())               │
[rank4]: │ ❱  995 │   │   │   tokenized_dataset = dataset.map(                          │
[rank4]: │    996 │   │   │   │   dataset_mapper,                                       │
[rank4]: │    997 │   │   │   │   batched=False,                                        │
[rank4]: │    998 │   │   │   │   remove_columns=columns_to_remove,                     │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:602 in wrapper      │
[rank4]: │                                                                              │
[rank4]: │    599 │   │   else:                                                         │
[rank4]: │    600 │   │   │   self: "Dataset" = kwargs.pop("self")                      │
[rank4]: │    601 │   │   # apply actual function                                       │
[rank4]: │ ❱  602 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank4]: │    603 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank4]: │    604 │   │   for dataset in datasets:                                      │
[rank4]: │    605 │   │   │   # Remove task templates if a column mapping of the templa │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:567 in wrapper      │
[rank4]: │                                                                              │
[rank4]: │    564 │   │   │   "output_all_columns": self._output_all_columns,           │
[rank4]: │    565 │   │   }                                                             │
[rank4]: │    566 │   │   # apply actual function                                       │
[rank4]: │ ❱  567 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank4]: │    568 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank4]: │    569 │   │   # re-apply format to the output                               │
[rank4]: │    570 │   │   for dataset in datasets:                                      │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:3253 in map         │
[rank4]: │                                                                              │
[rank4]: │   3250 │   │   │   │   │   │   total=pbar_total,                             │
[rank4]: │   3251 │   │   │   │   │   │   desc=(desc or "Map") + f" (num_proc={num_proc │
[rank4]: │   3252 │   │   │   │   │   ) as pbar:                                        │
[rank4]: │ ❱ 3253 │   │   │   │   │   │   for rank, done, content in iflatmap_unordered │
[rank4]: │   3254 │   │   │   │   │   │   │   pool, Dataset._map_single, kwargs_iterabl │
[rank4]: │   3255 │   │   │   │   │   │   ):                                            │
[rank4]: │   3256 │   │   │   │   │   │   │   if done:                                  │
[rank4]: │                                                                              │
[rank4]: │ /usr/lib/python3/dist-packages/datasets/utils/py_utils.py:711 in             │
[rank4]: │ iflatmap_unordered                                                           │
[rank4]: │                                                                              │
[rank4]: │   708 │   │   │   │   if _get_pool_pid(pool) != initial_pool_pid:            │
[rank4]: │   709 │   │   │   │   │   pool_changed = True                                │
[rank4]: │   710 │   │   │   │   │   # One of the subprocesses has died. We should not  │
[rank4]: │ ❱ 711 │   │   │   │   │   raise RuntimeError(                                │
[rank4]: │   712 │   │   │   │   │   │   "One of the subprocesses has abruptly died dur │
[rank4]: │   713 │   │   │   │   │   │   "To debug the error, disable multiprocessing." │
[rank4]: │   714 │   │   │   │   │   )                                                  │
[rank4]: ╰──────────────────────────────────────────────────────────────────────────────╯
[rank4]: RuntimeError: One of the subprocesses has abruptly died during map operation.To 
[rank4]: debug the error, disable multiprocessing.

----------End global rank 4 logs----------
Global rank 5 (PID 478) exited with code 1
----------Begin global rank 5 logs----------
/usr/lib/python3/dist-packages/transformer_engine/pytorch/attention.py:108: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
2024-12-28 09:25:04,640: rank5[478][MainThread]: DEBUG: llmfoundry.command_utils.train: Initializing dist with device...
2024-12-28 09:25:05,421: rank5[478][MainThread]: DEBUG: llmfoundry.command_utils.train: Testing barrier with device...
[rank5]:[W1228 09:25:05.459548353 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
2024-12-28 09:25:13,019: rank5[478][MainThread]: DEBUG: llmfoundry.command_utils.train: Barrier test passed with device.
2024-12-28 09:25:13,020: rank5[478][MainThread]: INFO: llmfoundry.command_utils.train: As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
/usr/lib/python3/dist-packages/llmfoundry/utils/config_utils.py:538: UserWarning: Setting `sync_module_states = True` for FSDP. This is required when using mixed initialization.
  warnings.warn((
2024-12-28 09:25:13,021: rank5[478][MainThread]: INFO: llmfoundry.command_utils.train: Building tokenizer...
2024-12-28 09:25:15,856: rank5[478][MainThread]: INFO: llmfoundry.command_utils.train: Building train loader...
2024-12-28 09:25:15,857: rank5[478][MainThread]: DEBUG: llmfoundry.data.packing: Searching for optimal packing ratio.
2024-12-28 09:25:15,933: rank5[478][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Using packing ratio 1.0
2024-12-28 09:25:15,933: rank5[478][MainThread]: INFO: llmfoundry.data.finetuning.tasks: No preprocessor was supplied and no preprocessing function is registered for dataset name "j316chuck/chess_rl". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.
2024-12-28 09:25:15,933: rank5[478][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Dataset constructor args {'decoder_only_format': True, 'hf_kwargs': {'data_dir': 'small'}, 'max_seq_len': 8192}
2024-12-28 09:25:15,937: rank5[478][MainThread]: DEBUG: llmfoundry.data.finetuning.tasks: Waiting for local_rank 0 to finish data prep

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]Process ForkPoolWorker-1:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int
Process ForkPoolWorker-2:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]
2024-12-28 09:25:34,749: rank5[478][MainThread]: ERROR: llmfoundry.data.finetuning.tasks: Error during data prep
[rank5]: ╭───────────────────── Traceback (most recent call last) ──────────────────────╮
[rank5]: │ /llm-foundry/scripts/train/train.py:9 in <module>                            │
[rank5]: │                                                                              │
[rank5]: │    6                                                                         │
[rank5]: │    7 if __name__ == '__main__':                                              │
[rank5]: │    8 │   yaml_path, args_list = sys.argv[1], sys.argv[2:]                    │
[rank5]: │ ❱  9 │   train_from_yaml(yaml_path, args_list)                               │
[rank5]: │   10                                                                         │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:662 in      │
[rank5]: │ train_from_yaml                                                              │
[rank5]: │                                                                              │
[rank5]: │   659 │   │   cli_cfg = om.from_cli(args_list)                               │
[rank5]: │   660 │   │   yaml_cfg = om.merge(yaml_cfg, cli_cfg)                         │
[rank5]: │   661 │   assert isinstance(yaml_cfg, DictConfig)                            │
[rank5]: │ ❱ 662 │   return train(yaml_cfg)                                             │
[rank5]: │   663                                                                        │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:457 in      │
[rank5]: │ train                                                                        │
[rank5]: │                                                                              │
[rank5]: │   454 │   # Dataloaders                                                      │
[rank5]: │   455 │   log.info('Building train loader...')                               │
[rank5]: │   456 │   try:                                                               │
[rank5]: │ ❱ 457 │   │   train_loader = build_dataloader(                               │
[rank5]: │   458 │   │   │   train_loader_config,                                       │
[rank5]: │   459 │   │   │   tokenizer,                                                 │
[rank5]: │   460 │   │   │   train_cfg.device_train_batch_size,                         │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank5]: │ build_dataloader                                                             │
[rank5]: │                                                                              │
[rank5]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank5]: │   37 │   }                                                                   │
[rank5]: │   38 │                                                                       │
[rank5]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank5]: │   40 │   │   name=name,                                                      │
[rank5]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank5]: │   42 │   │   partial_function=False,                                         │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank5]: │ construct_from_registry                                                      │
[rank5]: │                                                                              │
[rank5]: │   157 │   │   registered_constructor,                                        │
[rank5]: │   158 │   │   type,                                                          │
[rank5]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank5]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank5]: │   161 │   elif callable(registered_constructor):                             │
[rank5]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank5]: │   163 │   else:                                                              │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:232  │
[rank5]: │ in build_finetuning_dataloader                                               │
[rank5]: │                                                                              │
[rank5]: │   229 │   │   },                                                             │
[rank5]: │   230 │   )                                                                  │
[rank5]: │   231 │                                                                      │
[rank5]: │ ❱ 232 │   collate_fn, dataloader_batch_size = construct_from_registry(       │
[rank5]: │   233 │   │   name='finetuning_collator',                                    │
[rank5]: │   234 │   │   registry=registry.collators,                                   │
[rank5]: │   235 │   │   partial_function=False,                                        │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank5]: │ construct_from_registry                                                      │
[rank5]: │                                                                              │
[rank5]: │   157 │   │   registered_constructor,                                        │
[rank5]: │   158 │   │   type,                                                          │
[rank5]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank5]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank5]: │   161 │   elif callable(registered_constructor):                             │
[rank5]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank5]: │   163 │   else:                                                              │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/utils.py:239 in               │
[rank5]: │ get_finetuning_collator                                                      │
[rank5]: │                                                                              │
[rank5]: │   236 │   dataset_batch_size: int,                                           │
[rank5]: │   237 ) -> tuple[Union[Seq2SeqFinetuningCollator, BinPackCollator,           │
[rank5]: │   238 │   │   │   │    LossGeneratingTokensCollatorWrapper], int]:           │
[rank5]: │ ❱ 239 │   collate_fn, dataset_batch_size = build_collate_fn(                 │
[rank5]: │   240 │   │   dataloader_cfg,                                                │
[rank5]: │   241 │   │   tokenizer,                                                     │
[rank5]: │   242 │   │   dataset_batch_size,                                            │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:671  │
[rank5]: │ in build_collate_fn                                                          │
[rank5]: │                                                                              │
[rank5]: │   668 │   │   return collate_fn, device_batch_size                           │
[rank5]: │   669 │                                                                      │
[rank5]: │   670 │   if packing_ratio == 'auto':                                        │
[rank5]: │ ❱ 671 │   │   packing_ratio = auto_packing_ratio(                            │
[rank5]: │   672 │   │   │   dataloader_cfg=dataloader_cfg,                             │
[rank5]: │   673 │   │   │   tokenizer=tokenizer,                                       │
[rank5]: │   674 │   │   │   device_batch_size=device_batch_size,                       │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:411 in             │
[rank5]: │ auto_packing_ratio                                                           │
[rank5]: │                                                                              │
[rank5]: │   408 │   # Obtain the maximum packing_ratio/minimum padding that has no was │
[rank5]: │   409 │   # profiling_results are sorted from smallest to largest packing_ra │
[rank5]: │   410 │   packing_ratio = 1                                                  │
[rank5]: │ ❱ 411 │   for packing_ratio_candidate, _, waste in profiling_results:        │
[rank5]: │   412 │   │   if waste is None or waste > 0:                                 │
[rank5]: │   413 │   │   │   break                                                      │
[rank5]: │   414 │   │   packing_ratio = packing_ratio_candidate                        │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:509 in             │
[rank5]: │ profile_packing                                                              │
[rank5]: │                                                                              │
[rank5]: │   506 │                                                                      │
[rank5]: │   507 │   n_profile_examples = max(raw_batch_sizes) * 100                    │
[rank5]: │   508 │                                                                      │
[rank5]: │ ❱ 509 │   train_dataspec = build_dataloader(                                 │
[rank5]: │   510 │   │   dataloader_cfg,                                                │
[rank5]: │   511 │   │   tokenizer,                                                     │
[rank5]: │   512 │   │   n_profile_examples,                                            │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank5]: │ build_dataloader                                                             │
[rank5]: │                                                                              │
[rank5]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank5]: │   37 │   }                                                                   │
[rank5]: │   38 │                                                                       │
[rank5]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank5]: │   40 │   │   name=name,                                                      │
[rank5]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank5]: │   42 │   │   partial_function=False,                                         │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank5]: │ construct_from_registry                                                      │
[rank5]: │                                                                              │
[rank5]: │   157 │   │   registered_constructor,                                        │
[rank5]: │   158 │   │   type,                                                          │
[rank5]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank5]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank5]: │   161 │   elif callable(registered_constructor):                             │
[rank5]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank5]: │   163 │   else:                                                              │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:306  │
[rank5]: │ in build_finetuning_dataloader                                               │
[rank5]: │                                                                              │
[rank5]: │   303 │   │   │   k not in {'split', 'preprocessing_fn'}                     │
[rank5]: │   304 │   │   }                                                              │
[rank5]: │   305 │   │   log.info("Dataset constructor args %s", dataset_constructor_ar │
[rank5]: │ ❱ 306 │   │   streaming_dataset = dataset_constructor.build_from_hf(         │
[rank5]: │   307 │   │   │   dataset_name=dataset_name_or_path,                         │
[rank5]: │   308 │   │   │   split=split,                                               │
[rank5]: │   309 │   │   │   preprocessing_fn=preprocessing_fn,                         │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:1051 in   │
[rank5]: │ build_from_hf                                                                │
[rank5]: │                                                                              │
[rank5]: │   1048 │   │   │   ) from error                                              │
[rank5]: │   1049 │   │   if error is not None:                                         │
[rank5]: │   1050 │   │   │   log.error('Error during data prep')                       │
[rank5]: │ ❱ 1051 │   │   │   raise error                                               │
[rank5]: │   1052 │   │   log.debug('All ranks finished data prep')                     │
[rank5]: │   1053 │   │                                                                 │
[rank5]: │   1054 │   │   hf_tokenization_logger.removeFilter(sequence_length_warning_f │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:995 in    │
[rank5]: │ build_from_hf                                                                │
[rank5]: │                                                                              │
[rank5]: │    992 │   │   │   │   num_proc = 1                                          │
[rank5]: │    993 │   │   │                                                             │
[rank5]: │    994 │   │   │   columns_to_remove = list(dataset[0].keys())               │
[rank5]: │ ❱  995 │   │   │   tokenized_dataset = dataset.map(                          │
[rank5]: │    996 │   │   │   │   dataset_mapper,                                       │
[rank5]: │    997 │   │   │   │   batched=False,                                        │
[rank5]: │    998 │   │   │   │   remove_columns=columns_to_remove,                     │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:602 in wrapper      │
[rank5]: │                                                                              │
[rank5]: │    599 │   │   else:                                                         │
[rank5]: │    600 │   │   │   self: "Dataset" = kwargs.pop("self")                      │
[rank5]: │    601 │   │   # apply actual function                                       │
[rank5]: │ ❱  602 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank5]: │    603 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank5]: │    604 │   │   for dataset in datasets:                                      │
[rank5]: │    605 │   │   │   # Remove task templates if a column mapping of the templa │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:567 in wrapper      │
[rank5]: │                                                                              │
[rank5]: │    564 │   │   │   "output_all_columns": self._output_all_columns,           │
[rank5]: │    565 │   │   }                                                             │
[rank5]: │    566 │   │   # apply actual function                                       │
[rank5]: │ ❱  567 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank5]: │    568 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank5]: │    569 │   │   # re-apply format to the output                               │
[rank5]: │    570 │   │   for dataset in datasets:                                      │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:3253 in map         │
[rank5]: │                                                                              │
[rank5]: │   3250 │   │   │   │   │   │   total=pbar_total,                             │
[rank5]: │   3251 │   │   │   │   │   │   desc=(desc or "Map") + f" (num_proc={num_proc │
[rank5]: │   3252 │   │   │   │   │   ) as pbar:                                        │
[rank5]: │ ❱ 3253 │   │   │   │   │   │   for rank, done, content in iflatmap_unordered │
[rank5]: │   3254 │   │   │   │   │   │   │   pool, Dataset._map_single, kwargs_iterabl │
[rank5]: │   3255 │   │   │   │   │   │   ):                                            │
[rank5]: │   3256 │   │   │   │   │   │   │   if done:                                  │
[rank5]: │                                                                              │
[rank5]: │ /usr/lib/python3/dist-packages/datasets/utils/py_utils.py:711 in             │
[rank5]: │ iflatmap_unordered                                                           │
[rank5]: │                                                                              │
[rank5]: │   708 │   │   │   │   if _get_pool_pid(pool) != initial_pool_pid:            │
[rank5]: │   709 │   │   │   │   │   pool_changed = True                                │
[rank5]: │   710 │   │   │   │   │   # One of the subprocesses has died. We should not  │
[rank5]: │ ❱ 711 │   │   │   │   │   raise RuntimeError(                                │
[rank5]: │   712 │   │   │   │   │   │   "One of the subprocesses has abruptly died dur │
[rank5]: │   713 │   │   │   │   │   │   "To debug the error, disable multiprocessing." │
[rank5]: │   714 │   │   │   │   │   )                                                  │
[rank5]: ╰──────────────────────────────────────────────────────────────────────────────╯
[rank5]: RuntimeError: One of the subprocesses has abruptly died during map operation.To 
[rank5]: debug the error, disable multiprocessing.

----------End global rank 5 logs----------
Global rank 6 (PID 479) exited with code 1
----------Begin global rank 6 logs----------
/usr/lib/python3/dist-packages/transformer_engine/pytorch/attention.py:108: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
2024-12-28 09:25:04,606: rank6[479][MainThread]: DEBUG: llmfoundry.command_utils.train: Initializing dist with device...
2024-12-28 09:25:05,421: rank6[479][MainThread]: DEBUG: llmfoundry.command_utils.train: Testing barrier with device...
[rank6]:[W1228 09:25:05.459520367 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
2024-12-28 09:25:13,019: rank6[479][MainThread]: DEBUG: llmfoundry.command_utils.train: Barrier test passed with device.
2024-12-28 09:25:13,020: rank6[479][MainThread]: INFO: llmfoundry.command_utils.train: As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
/usr/lib/python3/dist-packages/llmfoundry/utils/config_utils.py:538: UserWarning: Setting `sync_module_states = True` for FSDP. This is required when using mixed initialization.
  warnings.warn((
2024-12-28 09:25:13,021: rank6[479][MainThread]: INFO: llmfoundry.command_utils.train: Building tokenizer...
2024-12-28 09:25:15,864: rank6[479][MainThread]: INFO: llmfoundry.command_utils.train: Building train loader...
2024-12-28 09:25:15,865: rank6[479][MainThread]: DEBUG: llmfoundry.data.packing: Searching for optimal packing ratio.
2024-12-28 09:25:15,933: rank6[479][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Using packing ratio 1.0
2024-12-28 09:25:15,933: rank6[479][MainThread]: INFO: llmfoundry.data.finetuning.tasks: No preprocessor was supplied and no preprocessing function is registered for dataset name "j316chuck/chess_rl". No additional preprocessing will be applied. If the dataset is already formatted correctly, you can ignore this message.
2024-12-28 09:25:15,933: rank6[479][MainThread]: INFO: llmfoundry.data.finetuning.dataloader: Dataset constructor args {'decoder_only_format': True, 'hf_kwargs': {'data_dir': 'small'}, 'max_seq_len': 8192}
2024-12-28 09:25:15,936: rank6[479][MainThread]: DEBUG: llmfoundry.data.finetuning.tasks: Waiting for local_rank 0 to finish data prep

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]Process ForkPoolWorker-1:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int
Process ForkPoolWorker-2:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3/dist-packages/multiprocess/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3/dist-packages/multiprocess/pool.py", line 114, in worker
    task = get()
           ^^^^^
  File "/usr/lib/python3/dist-packages/multiprocess/queues.py", line 370, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 275, in loads
    return load(file, ignore, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3/dist-packages/dill/_dill.py", line 472, in load
    obj = StockUnpickler.load(self)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: code() argument 13 must be str, not int

Tokenizing dataset (num_proc=216):   0%|          | 0/500000 [00:00<?, ? examples/s]
2024-12-28 09:25:34,749: rank6[479][MainThread]: ERROR: llmfoundry.data.finetuning.tasks: Error during data prep
[rank6]: ╭───────────────────── Traceback (most recent call last) ──────────────────────╮
[rank6]: │ /llm-foundry/scripts/train/train.py:9 in <module>                            │
[rank6]: │                                                                              │
[rank6]: │    6                                                                         │
[rank6]: │    7 if __name__ == '__main__':                                              │
[rank6]: │    8 │   yaml_path, args_list = sys.argv[1], sys.argv[2:]                    │
[rank6]: │ ❱  9 │   train_from_yaml(yaml_path, args_list)                               │
[rank6]: │   10                                                                         │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:662 in      │
[rank6]: │ train_from_yaml                                                              │
[rank6]: │                                                                              │
[rank6]: │   659 │   │   cli_cfg = om.from_cli(args_list)                               │
[rank6]: │   660 │   │   yaml_cfg = om.merge(yaml_cfg, cli_cfg)                         │
[rank6]: │   661 │   assert isinstance(yaml_cfg, DictConfig)                            │
[rank6]: │ ❱ 662 │   return train(yaml_cfg)                                             │
[rank6]: │   663                                                                        │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/command_utils/train.py:457 in      │
[rank6]: │ train                                                                        │
[rank6]: │                                                                              │
[rank6]: │   454 │   # Dataloaders                                                      │
[rank6]: │   455 │   log.info('Building train loader...')                               │
[rank6]: │   456 │   try:                                                               │
[rank6]: │ ❱ 457 │   │   train_loader = build_dataloader(                               │
[rank6]: │   458 │   │   │   train_loader_config,                                       │
[rank6]: │   459 │   │   │   tokenizer,                                                 │
[rank6]: │   460 │   │   │   train_cfg.device_train_batch_size,                         │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank6]: │ build_dataloader                                                             │
[rank6]: │                                                                              │
[rank6]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank6]: │   37 │   }                                                                   │
[rank6]: │   38 │                                                                       │
[rank6]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank6]: │   40 │   │   name=name,                                                      │
[rank6]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank6]: │   42 │   │   partial_function=False,                                         │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank6]: │ construct_from_registry                                                      │
[rank6]: │                                                                              │
[rank6]: │   157 │   │   registered_constructor,                                        │
[rank6]: │   158 │   │   type,                                                          │
[rank6]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank6]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank6]: │   161 │   elif callable(registered_constructor):                             │
[rank6]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank6]: │   163 │   else:                                                              │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:232  │
[rank6]: │ in build_finetuning_dataloader                                               │
[rank6]: │                                                                              │
[rank6]: │   229 │   │   },                                                             │
[rank6]: │   230 │   )                                                                  │
[rank6]: │   231 │                                                                      │
[rank6]: │ ❱ 232 │   collate_fn, dataloader_batch_size = construct_from_registry(       │
[rank6]: │   233 │   │   name='finetuning_collator',                                    │
[rank6]: │   234 │   │   registry=registry.collators,                                   │
[rank6]: │   235 │   │   partial_function=False,                                        │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank6]: │ construct_from_registry                                                      │
[rank6]: │                                                                              │
[rank6]: │   157 │   │   registered_constructor,                                        │
[rank6]: │   158 │   │   type,                                                          │
[rank6]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank6]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank6]: │   161 │   elif callable(registered_constructor):                             │
[rank6]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank6]: │   163 │   else:                                                              │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/utils.py:239 in               │
[rank6]: │ get_finetuning_collator                                                      │
[rank6]: │                                                                              │
[rank6]: │   236 │   dataset_batch_size: int,                                           │
[rank6]: │   237 ) -> tuple[Union[Seq2SeqFinetuningCollator, BinPackCollator,           │
[rank6]: │   238 │   │   │   │    LossGeneratingTokensCollatorWrapper], int]:           │
[rank6]: │ ❱ 239 │   collate_fn, dataset_batch_size = build_collate_fn(                 │
[rank6]: │   240 │   │   dataloader_cfg,                                                │
[rank6]: │   241 │   │   tokenizer,                                                     │
[rank6]: │   242 │   │   dataset_batch_size,                                            │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:671  │
[rank6]: │ in build_collate_fn                                                          │
[rank6]: │                                                                              │
[rank6]: │   668 │   │   return collate_fn, device_batch_size                           │
[rank6]: │   669 │                                                                      │
[rank6]: │   670 │   if packing_ratio == 'auto':                                        │
[rank6]: │ ❱ 671 │   │   packing_ratio = auto_packing_ratio(                            │
[rank6]: │   672 │   │   │   dataloader_cfg=dataloader_cfg,                             │
[rank6]: │   673 │   │   │   tokenizer=tokenizer,                                       │
[rank6]: │   674 │   │   │   device_batch_size=device_batch_size,                       │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:411 in             │
[rank6]: │ auto_packing_ratio                                                           │
[rank6]: │                                                                              │
[rank6]: │   408 │   # Obtain the maximum packing_ratio/minimum padding that has no was │
[rank6]: │   409 │   # profiling_results are sorted from smallest to largest packing_ra │
[rank6]: │   410 │   packing_ratio = 1                                                  │
[rank6]: │ ❱ 411 │   for packing_ratio_candidate, _, waste in profiling_results:        │
[rank6]: │   412 │   │   if waste is None or waste > 0:                                 │
[rank6]: │   413 │   │   │   break                                                      │
[rank6]: │   414 │   │   packing_ratio = packing_ratio_candidate                        │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/packing.py:509 in             │
[rank6]: │ profile_packing                                                              │
[rank6]: │                                                                              │
[rank6]: │   506 │                                                                      │
[rank6]: │   507 │   n_profile_examples = max(raw_batch_sizes) * 100                    │
[rank6]: │   508 │                                                                      │
[rank6]: │ ❱ 509 │   train_dataspec = build_dataloader(                                 │
[rank6]: │   510 │   │   dataloader_cfg,                                                │
[rank6]: │   511 │   │   tokenizer,                                                     │
[rank6]: │   512 │   │   n_profile_examples,                                            │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/dataloader.py:39 in           │
[rank6]: │ build_dataloader                                                             │
[rank6]: │                                                                              │
[rank6]: │   36 │   │   'device_batch_size': device_batch_size,                         │
[rank6]: │   37 │   }                                                                   │
[rank6]: │   38 │                                                                       │
[rank6]: │ ❱ 39 │   return construct_from_registry(                                     │
[rank6]: │   40 │   │   name=name,                                                      │
[rank6]: │   41 │   │   registry=registry.dataloaders,                                  │
[rank6]: │   42 │   │   partial_function=False,                                         │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/utils/registry_utils.py:160 in     │
[rank6]: │ construct_from_registry                                                      │
[rank6]: │                                                                              │
[rank6]: │   157 │   │   registered_constructor,                                        │
[rank6]: │   158 │   │   type,                                                          │
[rank6]: │   159 │   ) or callable(registered_constructor) and not partial_function:    │
[rank6]: │ ❱ 160 │   │   constructed_item = registered_constructor(**kwargs)            │
[rank6]: │   161 │   elif callable(registered_constructor):                             │
[rank6]: │   162 │   │   constructed_item = functools.partial(registered_constructor, * │
[rank6]: │   163 │   else:                                                              │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/dataloader.py:306  │
[rank6]: │ in build_finetuning_dataloader                                               │
[rank6]: │                                                                              │
[rank6]: │   303 │   │   │   k not in {'split', 'preprocessing_fn'}                     │
[rank6]: │   304 │   │   }                                                              │
[rank6]: │   305 │   │   log.info("Dataset constructor args %s", dataset_constructor_ar │
[rank6]: │ ❱ 306 │   │   streaming_dataset = dataset_constructor.build_from_hf(         │
[rank6]: │   307 │   │   │   dataset_name=dataset_name_or_path,                         │
[rank6]: │   308 │   │   │   split=split,                                               │
[rank6]: │   309 │   │   │   preprocessing_fn=preprocessing_fn,                         │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:1051 in   │
[rank6]: │ build_from_hf                                                                │
[rank6]: │                                                                              │
[rank6]: │   1048 │   │   │   ) from error                                              │
[rank6]: │   1049 │   │   if error is not None:                                         │
[rank6]: │   1050 │   │   │   log.error('Error during data prep')                       │
[rank6]: │ ❱ 1051 │   │   │   raise error                                               │
[rank6]: │   1052 │   │   log.debug('All ranks finished data prep')                     │
[rank6]: │   1053 │   │                                                                 │
[rank6]: │   1054 │   │   hf_tokenization_logger.removeFilter(sequence_length_warning_f │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/llmfoundry/data/finetuning/tasks.py:995 in    │
[rank6]: │ build_from_hf                                                                │
[rank6]: │                                                                              │
[rank6]: │    992 │   │   │   │   num_proc = 1                                          │
[rank6]: │    993 │   │   │                                                             │
[rank6]: │    994 │   │   │   columns_to_remove = list(dataset[0].keys())               │
[rank6]: │ ❱  995 │   │   │   tokenized_dataset = dataset.map(                          │
[rank6]: │    996 │   │   │   │   dataset_mapper,                                       │
[rank6]: │    997 │   │   │   │   batched=False,                                        │
[rank6]: │    998 │   │   │   │   remove_columns=columns_to_remove,                     │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:602 in wrapper      │
[rank6]: │                                                                              │
[rank6]: │    599 │   │   else:                                                         │
[rank6]: │    600 │   │   │   self: "Dataset" = kwargs.pop("self")                      │
[rank6]: │    601 │   │   # apply actual function                                       │
[rank6]: │ ❱  602 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank6]: │    603 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank6]: │    604 │   │   for dataset in datasets:                                      │
[rank6]: │    605 │   │   │   # Remove task templates if a column mapping of the templa │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:567 in wrapper      │
[rank6]: │                                                                              │
[rank6]: │    564 │   │   │   "output_all_columns": self._output_all_columns,           │
[rank6]: │    565 │   │   }                                                             │
[rank6]: │    566 │   │   # apply actual function                                       │
[rank6]: │ ❱  567 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
[rank6]: │    568 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
[rank6]: │    569 │   │   # re-apply format to the output                               │
[rank6]: │    570 │   │   for dataset in datasets:                                      │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/datasets/arrow_dataset.py:3253 in map         │
[rank6]: │                                                                              │
[rank6]: │   3250 │   │   │   │   │   │   total=pbar_total,                             │
[rank6]: │   3251 │   │   │   │   │   │   desc=(desc or "Map") + f" (num_proc={num_proc │
[rank6]: │   3252 │   │   │   │   │   ) as pbar:                                        │
[rank6]: │ ❱ 3253 │   │   │   │   │   │   for rank, done, content in iflatmap_unordered │
[rank6]: │   3254 │   │   │   │   │   │   │   pool, Dataset._map_single, kwargs_iterabl │
[rank6]: │   3255 │   │   │   │   │   │   ):                                            │
[rank6]: │   3256 │   │   │   │   │   │   │   if done:                                  │
[rank6]: │                                                                              │
[rank6]: │ /usr/lib/python3/dist-packages/datasets/utils/py_utils.py:711 in             │
[rank6]: │ iflatmap_unordered                                                           │
[rank6]: │                                                                              │
[rank6]: │   708 │   │   │   │   if _get_pool_pid(pool) != initial_pool_pid:            │
[rank6]: │   709 │   │   │   │   │   pool_changed = True                                │
[rank6]: │   710 │   │   │   │   │   # One of the subprocesses has died. We should not  │
[rank6]: │ ❱ 711 │   │   │   │   │   raise RuntimeError(                                │
[rank6]: │   712 │   │   │   │   │   │   "One of the subprocesses has abruptly died dur │
[rank6]: │   713 │   │   │   │   │   │   "To debug the error, disable multiprocessing." │
[rank6]: │   714 │   │   │   │   │   )                                                  │
[rank6]: ╰──────────────────────────────────────────────────────────────────────────────╯
[rank6]: RuntimeError: One of the subprocesses has abruptly died during map operation.To 
[rank6]: debug the error, disable multiprocessing.

----------End global rank 6 logs----------
ERROR:composer.cli.launcher:Global rank 0 (PID 473) exited with code 1
